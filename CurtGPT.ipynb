{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tim-a-davis/silly_little_language_modeling_thing_at_utd/blob/main/CurtGPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<table>\n",
        "<tr>\n",
        "<td style=\"width: 10%;\">\n",
        "\n",
        "# CurtGPT\n",
        "Using Microsoft's Phi 1.5 model like it was never intended.\n",
        "\n",
        "</td>\n",
        "<td style=\"text-align: center;\">\n",
        "<img src=\"https://github.com/tim-a-davis/silly_little_language_modeling_thing_at_utd/blob/main/curtgpt%20logo.png?raw=true\" width=\"300\" height=\"auto\">\n",
        "</td>\n",
        "</tr>\n",
        "</table>\n"
      ],
      "metadata": {
        "id": "AgN0tXTQ5SVS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup, Installs, Imports\n",
        "\n",
        "Setup of the environment, installation of the needed models, and importing everything required to run the notebook"
      ],
      "metadata": {
        "id": "G6hseI_DS0H2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Installing dependencies\n",
        "!pip install -q trl transformers accelerate peft datasets bitsandbytes einops"
      ],
      "metadata": {
        "id": "eJQmTHR3dIrL"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Imports & setup\n",
        "from IPython.display import HTML, display, clear_output\n",
        "import ipywidgets as widgets\n",
        "\n",
        "import requests\n",
        "import random\n",
        "import time\n",
        "import math\n",
        "import numba\n",
        "import numpy as np\n",
        "import itertools\n",
        "\n",
        "from collections import defaultdict, Counter\n",
        "from typing import List, Callable, Dict\n",
        "from pprint import pprint\n",
        "from einops import rearrange\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    GenerationConfig,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments\n",
        ")\n",
        "from trl import DPOTrainer\n",
        "from peft import AutoPeftModelForCausalLM, LoraConfig\n",
        "from datasets import load_dataset, Dataset\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "import seaborn as sns\n",
        "\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ],
      "metadata": {
        "id": "miP5UKcBae7u"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install Phi models and tokenizer\n",
        "AutoModelForCausalLM.from_pretrained(\"microsoft/phi-1_5\", trust_remote_code=True, torch_dtype=\"auto\")\n",
        "AutoTokenizer.from_pretrained(\"microsoft/phi-1_5\", trust_remote_code=True, torch_dtype=\"auto\")\n",
        "\n",
        "AutoModelForCausalLM.from_pretrained(\"teknium/Puffin-Phi-v2\", trust_remote_code=True, torch_dtype=torch.bfloat16)\n",
        "AutoTokenizer.from_pretrained(\"teknium/Puffin-Phi-v2\", trust_remote_code=True, torch_dtype=torch.bfloat16)\n",
        "\n",
        "#torch.set_default_device('cuda')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "HzJsUkpeTab4",
        "outputId": "18a632d9-ee2d-4b49-85ae-19837d70c8e4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CodeGenTokenizerFast(name_or_path='teknium/Puffin-Phi-v2', vocab_size=50257, model_max_length=2048, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# N-Gram Models\n",
        "\n",
        "Starting with n-gram models will hopfully build some intuition on language modeling in general."
      ],
      "metadata": {
        "id": "_pwlS5rkTAIh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building the Trigram Model\n",
        "---\n",
        "Here we write all the code we'll need to ingest a corpus of text and create a representation of that text in the form of a trigram model.  The main idea behind the trigram model is fundementally the same as with decoder-only tranformer based models, but trigram models are easier to disect, so we'll start there."
      ],
      "metadata": {
        "id": "lX2tBK0fUZRC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TrigramModel:\n",
        "    def __init__(self, url):\n",
        "        self.trigram_freq = defaultdict(Counter)\n",
        "        self._train(url)\n",
        "\n",
        "    def _train(self, url):\n",
        "        r = requests.get(url)\n",
        "        text = r.text.lower().split()\n",
        "\n",
        "        # Create trigrams\n",
        "        for i in range(len(text) - 2):\n",
        "            trigram = (text[i], text[i + 1], text[i + 2])\n",
        "            self.trigram_freq[(trigram[0], trigram[1])][trigram[2]] += 1\n",
        "\n",
        "    def _get_weighted_random_word(self, counter):\n",
        "        total = sum(counter.values())\n",
        "        random_choice = random.randint(1, total)\n",
        "\n",
        "        for word, freq in counter.items():\n",
        "            random_choice -= freq\n",
        "            if random_choice <= 0:\n",
        "                return word\n",
        "\n",
        "    def predict(self, text, n_words):\n",
        "        words = text.lower().split()\n",
        "        output = words.copy()\n",
        "\n",
        "        for _ in range(n_words):\n",
        "            last_bigram = tuple(output[-2:])\n",
        "            if last_bigram in self.trigram_freq:\n",
        "                next_word = self._get_weighted_random_word(\n",
        "                    self.trigram_freq[last_bigram]\n",
        "                )\n",
        "                output.append(next_word)\n",
        "            else:\n",
        "                break\n",
        "\n",
        "        return \" \".join(output)\n",
        "\n",
        "    def get_frequencies_of_bigram(self, text):\n",
        "        words = text.lower().split()\n",
        "        bigram = tuple(words[-2:])\n",
        "        return bigram, self.trigram_freq[bigram]\n"
      ],
      "metadata": {
        "id": "PTRO7WnoagbC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38
        },
        "outputId": "b06431d4-c3d1-4ec1-b9ca-c8520e5d99f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Instantiate The Model\n",
        "---\n",
        "\n",
        "Let's instantiate this trigram model with a .txt file containing the book _Billy Budd, Sailor_ by _Herman Melville_"
      ],
      "metadata": {
        "id": "y7Ok1o9GU0fj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = TrigramModel(\"http://gutenberg.net.au/ebooks06/0608511.txt\")"
      ],
      "metadata": {
        "id": "7-eYXbtwg8_m",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38
        },
        "outputId": "c6ed1140-e472-40e2-acfa-7f9ce64bcacd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "Here we prompt this model with some starting text, and we want to see what the model things the next n_words will be.  Given the way we've tokenized the text (splitting on spaces), the trigram model will only be looking at the last 2 words (in this case `the, master-at-arms`).  Then we add some small delay to make it look like a sweet streaming GPT model"
      ],
      "metadata": {
        "id": "yFYADHGCVHuq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"as it started to sway, the master-at-arms\"\n",
        "n_words = 50  # Number of words ahead to predict\n",
        "\n",
        "prediction = model.predict(prompt, n_words)\n",
        "for i, letter in enumerate(prediction):\n",
        "    if not i % 100: print(\"\\n\")\n",
        "    print(letter, end='', flush=True)\n",
        "    time.sleep(0.003)"
      ],
      "metadata": {
        "id": "AW--CcH2hILO",
        "outputId": "e0f487e9-7cb1-4bed-f78c-a7e621bf698a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "as it started to sway, the master-at-arms and the old-fashioned sailor, the commander of the heart n\n",
            "\n",
            "ot the less to do quite as much as he was everything that a young man if of the honest sense of fear\n",
            "\n",
            ", his apprehension as to the gazer's professional eye it was the most important regards ceased to be\n",
            "\n",
            " called,"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Disecting one of the bigrams\n",
        "\n",
        "---\n",
        "\n",
        "Here we can take a look at the models choice of words for our example bigram.  The output shows the bigram, as well as the frequencies of words found in the text.  "
      ],
      "metadata": {
        "id": "oPwJRKi6Vv1T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.get_frequencies_of_bigram(prompt)"
      ],
      "metadata": {
        "id": "MGT9pkrthKSO",
        "outputId": "28e656e3-b795-4d32-b385-48843744366d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 798
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(('the', 'master-at-arms'),\n",
              " Counter({'of': 1,\n",
              "          'was': 4,\n",
              "          'has': 1,\n",
              "          'in': 1,\n",
              "          'noticed': 1,\n",
              "          'that': 1,\n",
              "          'never': 1,\n",
              "          'being': 1,\n",
              "          'acted': 1,\n",
              "          'about': 1,\n",
              "          'said.': 1,\n",
              "          'said': 1,\n",
              "          'as': 1,\n",
              "          'and': 1}))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Confusing diagram"
      ],
      "metadata": {
        "id": "EYKl9KV7WC-s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![overly-complicated-diagram](http://www.phon.ox.ac.uk/jcoleman/old_SLP/Lecture_6/figure7-8.png)\n",
        "\n",
        "http://www.phon.ox.ac.uk/jcoleman/old_SLP/Lecture_6/trigram-modelling.html"
      ],
      "metadata": {
        "id": "3M4k-xVvk-b3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Base Transformer Model -- Phi-1.5\n",
        "\n",
        "---\n",
        "\n",
        "Here we'll start to look at a base transformer model to understand a little bit about how it behaves, how it's similar to a trigram model, and how we can use some tools to easily interact with these extremely large models."
      ],
      "metadata": {
        "id": "gRPuQCI_UM7F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Downloading and Instantiating\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "This part is not difficult thanks to the great work at huggingface.  With just two lines of code we can download a 1.3 billion parameter transformer model, map the weights onto the architecture, and load the associated configurations and tokenizers."
      ],
      "metadata": {
        "id": "mbRGL_SJWZzg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-1_5\", trust_remote_code=True, torch_dtype=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-1_5\", trust_remote_code=True, torch_dtype=\"auto\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38
        },
        "id": "IDMwICNuN3mS",
        "outputId": "217c4f7e-f53d-459e-b6ae-0fd765da9730"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompting Phi 1.5\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Here we take our same prompt, and we use the model's tokenizer to turn it into integers that the model can ingest.\n",
        "The result will be a string of integers that represent chunks of text we call tokens."
      ],
      "metadata": {
        "id": "HJ2ta6FjW2FI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"as it started to sway, the master-at-arms\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\", return_attention_mask=False)\n",
        "print(inputs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "id": "5kjTm5iSWXSc",
        "outputId": "e0f89e1f-8c0f-4711-a26d-9c1a2158415a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[  292,   340,  2067,   284, 20009,    11,   262,  4958,    12,   265,\n",
            "            12,  8357]], device='cuda:0')}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "We can use the tokenizer to reverse the process and get back the strings.\n",
        "You can see that the tokenizer sometimes chooses interesting places to chunk the text.\n",
        "You can also see that generally speaking, more common tokens have lower integer values.\n",
        "\n",
        "Here we show the integer values of each token and what the string representation is for each input."
      ],
      "metadata": {
        "id": "YtgrA9f0W92D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for token_id in inputs[\"input_ids\"][0]:\n",
        "    id = token_id.item()\n",
        "    token = tokenizer.decode(id)\n",
        "    print(f\"{id: <5} ----> {token}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "pjE2ZcWPtC3C",
        "outputId": "6e1ea623-7759-4080-cef6-6bf4aa0df847"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "292   ----> as\n",
            "340   ---->  it\n",
            "2067  ---->  started\n",
            "284   ---->  to\n",
            "20009 ---->  sway\n",
            "11    ----> ,\n",
            "262   ---->  the\n",
            "4958  ---->  master\n",
            "12    ----> -\n",
            "265   ----> at\n",
            "12    ----> -\n",
            "8357  ----> arms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Performing inference on this model is as simple as passing in the inputs from the tokenizer to the .generate method.\n",
        "\n",
        "The tokenizer has a batch_decode method that we would generally use to get back an output text.  But in this case\n",
        "we want to see each individual token and what the model output was.\n",
        "\n"
      ],
      "metadata": {
        "id": "WrZRnxhQXDp4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(**inputs, max_new_tokens=11)\n",
        "output_tokens = [tokenizer.decode(id) for id in outputs[0]]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 18
        },
        "id": "LCNfBF_0WXUX",
        "outputId": "d5cabd32-c7be-4e2e-d615-f87cd9465a8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Helper function for printing token ids and tokens\n",
        "def print_tokens(ids, tokens, line_size=18):\n",
        "    tokens = [token.replace(\" \", \"·\") for token in tokens]\n",
        "    def chunk_list(lst, max_size):\n",
        "        for i in range(0, len(lst), max_size):\n",
        "            yield lst[i:i + max_size]\n",
        "    id_chunks = list(chunk_list(ids, line_size))\n",
        "    token_chunks = list(chunk_list(tokens, line_size))\n",
        "    for ids, tokens in zip(id_chunks, token_chunks):\n",
        "        max_widths = [max(len(str(id)), len(token)) for id, token in zip(ids, tokens)]\n",
        "        aligned_ids = [str(id).center(max_widths[i]) for i, id in enumerate(ids)]\n",
        "        aligned_arrows = ['↓'.center(max_widths[i]) for i in range(len(ids))]\n",
        "        aligned_tokens = [token.center(max_widths[i]) for i, token in enumerate(tokens)]\n",
        "        print(' '.join(aligned_ids))\n",
        "        print(' '.join(aligned_arrows))\n",
        "        print(repr(' '.join(aligned_tokens))[1:-1])\n",
        "        print(\"\\n\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 18
        },
        "cellView": "form",
        "id": "4JHmujrpqUh-",
        "outputId": "e7948eb2-1c65-494a-982e-3a38e015b48b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inspecting Model Outputs\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Here we can decode each token output from the model, and map it back to original text.  We can see that this output is much more congruous with the input prompt over the trigram model.  "
      ],
      "metadata": {
        "id": "POcTMiJGXNs3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"Output:\\n\" + \"\".join(output_tokens) + \"\\n\\nToken Mapping:\")\n",
        "print_tokens(outputs.cpu().tolist()[0], output_tokens)\n",
        "\n",
        "print(\"\\n\\n* The · characters represent spaces in the token\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "iHZ_Zp_wcr-7",
        "outputId": "cd90d214-9113-459d-8299-19c35382389e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "as it started to sway, the master-at-arms, a seasoned veteran of the battlefield, stepped forward.\n",
            "\n",
            "Token Mapping:\n",
            "292 340   2067   284 20009 11 262    4958  12 265 12 8357 11 257   29314     9298   286 262     13480     11  10764     2651   13\n",
            " ↓   ↓     ↓      ↓    ↓   ↓   ↓      ↓    ↓   ↓  ↓   ↓   ↓   ↓      ↓        ↓      ↓   ↓        ↓       ↓     ↓        ↓     ↓ \n",
            " as ·it ·started ·to ·sway ,  ·the ·master -   at -  arms ,   ·a ·seasoned ·veteran ·of ·the ·battlefield ,  ·stepped ·forward . \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "* The · characters represent spaces in the token\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "The model itself outputs a tensor of size (..., sequence_length, vocabulary_size).  In this model, the vocab size is 51200.\n",
        "Each token in the vocabulary is assigned a value that is roughly the probability of that value being next in the sequence.\n",
        "We can see for our sequence what the top ten next tokens were by finding the tokens with the highest values.\n"
      ],
      "metadata": {
        "id": "J7Ca4c6NXk3u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  single_forward_pass = model.forward(**inputs) # perform one forward pass\n",
        "\n",
        "print(f\"Shape of outputs: {single_forward_pass.logits.shape}\\n\\n\")\n",
        "\n",
        "logits = single_forward_pass.logits[0, -1, :].cpu()\n",
        "exp_sum = torch.exp(single_forward_pass.logits[0, -1, :].cpu()).sum().item()\n",
        "top_10_token_ids = single_forward_pass.logits[0, -1, :].cpu().argsort().tolist()[-10:][::-1] # get the top 10 tokens in the vocabulary from the output tensor\n",
        "top_10_tokens = [tokenizer.decode(token) for token in top_10_token_ids] # decode them back to strings\n",
        "top_10_probs = (torch.exp(single_forward_pass.logits[0, -1, :].cpu()[top_10_token_ids])/exp_sum).tolist() # get their probabilities from the output of the model\n",
        "top_10_probs_rounded = [round(i, 5) for i in top_10_probs]\n",
        "\n",
        "\n",
        "print(\"Top 10 next possible tokens given our input:\\n\")\n",
        "print(\"token            probability\")\n",
        "print(\"-\"*28)\n",
        "for token, prob in zip(top_10_tokens, top_10_probs_rounded):\n",
        "    print(f\"{repr(token)[1:-1]: <10} ----> {prob: >11}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "2SbW1CCX76AQ",
        "outputId": "31bf6f3d-6579-4e81-90ab-0e85eb2d672b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of outputs: torch.Size([1, 12, 51200])\n",
            "\n",
            "\n",
            "Top 10 next possible tokens given our input:\n",
            "\n",
            "token            probability\n",
            "----------------------------\n",
            ",          ---->     0.04116\n",
            " quickly   ---->     0.03307\n",
            " knew      ---->     0.02873\n",
            " and       ---->     0.02575\n",
            " couldn    ---->     0.02575\n",
            " swiftly   ---->     0.02203\n",
            " skill     ---->     0.01959\n",
            " decided   ---->     0.01798\n",
            " took      ---->     0.01676\n",
            " of        ---->     0.01491\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A great overview of the different sampling algorithms commonly found for LLMs can be found [here](https://towardsdatascience.com/decoding-strategies-that-you-need-to-know-for-response-generation-ba95ee0faadc)"
      ],
      "metadata": {
        "id": "njuAs8nOBhO9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Writing a custom sampler\n",
        "---\n",
        "Decoder-only models produce vocabulary-length array of logits that we can transform into probabilities using a softmax function.  Softmax is defines as:\n",
        "\n",
        "$$\n",
        "\\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j=1}^{n} e^{x_j}}\n",
        "$$\n",
        "\n",
        "Typically, to constrain the range of possible outcomes, samplers use a parameter called temperature to adjust the probability space.  By dividing each logit by T, it makes more probable tokens even _more_ probable, and less probable tokens even _less_ probable. The softmax function with temperature looks like this:\n",
        "\n",
        "$$\n",
        "\\text{softmax}(x_i; T) = \\frac{e^{x_i / T}}{\\sum_{j=1}^{n} e^{x_j / T}}\n",
        "$$\n",
        "\n",
        "\n",
        "You can see then that the base softmax function has a \"default\" temperature of 1.  The following illustration shows how changing the temperature can effect the probabilities:\n",
        "\n",
        "![temp](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i9cXkz-TWG7-BS6CycahuQ.png)"
      ],
      "metadata": {
        "id": "l_dbLJLQZ2Nq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_possible_tokens(prompt, temperature=1, top_p=0.9, top_k=None):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", return_attention_mask=False)\n",
        "    with torch.no_grad():\n",
        "        single_forward_pass = model.forward(**inputs)\n",
        "    logits = single_forward_pass.logits[0, -1, :].cpu() # get the logits from the outputs\n",
        "    logits = logits - torch.max(logits) # subtract the max for numerical stability\n",
        "    exp_sum = torch.exp(logits/temperature).sum().item() # find the sum of the exponentiated logits\n",
        "    probs = torch.exp(logits/temperature)/exp_sum # get the probability from softmax\n",
        "    sort_idx = probs.argsort().tolist()[::-1] # get the index of tokens from most probable to least probable\n",
        "    probs_sorted = probs[sort_idx] # sort the probabilities\n",
        "    top_k_calculated = ((torch.cumsum(probs_sorted, 0) > top_p) * 1).argmax() + 1 # find the index of the token that crosses the top_p threshold\n",
        "    top_k = top_k or top_k_calculated # override top_k if it is passed as a kwarg\n",
        "    token_ids_considered = sort_idx[:top_k] # get the list of tokens where sum(p) < top_p\n",
        "    token_probs = (probs_sorted[:top_k] / torch.sum(probs_sorted[:top_k])).to(torch.float16)\n",
        "    tokens_considered = tokenizer.batch_decode(token_ids_considered)\n",
        "    return tokens_considered, token_probs\n",
        "\n",
        "\n",
        "def generate_custom(prompt: str,\n",
        "                    custom_sampler: Callable,\n",
        "                    temperature: float=1,\n",
        "                    top_p: float=0.9,\n",
        "                    top_k: float=None,\n",
        "                    max_new_tokens: int=20,\n",
        "                    **kwargs\n",
        "    ):\n",
        "    for _ in range(max_new_tokens):\n",
        "        tokens_considered, token_probs = get_possible_tokens(prompt, temperature=temperature, top_p=top_p, top_k=top_k)\n",
        "        selected_token = custom_sampler(tokens_considered, token_probs, **kwargs)\n",
        "        prompt += selected_token\n",
        "    return prompt\n",
        "\n",
        "\n",
        "def try_for_certain_letter(tokens: List, token_probs: torch.Tensor, letter: str=\"b\") -> str:\n",
        "    \"\"\"\n",
        "    Given a list of token strings and corresponding probabilities, this function returns a token that starts\n",
        "    with the letter -letter- if any such token exists in the list. The token must also follow a space, meaning it should\n",
        "    represent the beginning of a new word. If no such token is found, a token is selected randomly from the list\n",
        "    based on the given probabilities.\n",
        "\n",
        "    Parameters:\n",
        "    - tokens (list of str): A list of tokens (substrings) to search through.\n",
        "    - token_probs (numpy.ndarray or tensor): An array or tensor of probabilities corresponding to each token in `tokens`.\n",
        "                                              The length of this array should match the length of `tokens`.\n",
        "\n",
        "    Returns:\n",
        "    - str: A token string. If a token starts with the letter -letter- and is the beginning of a new word (i.e., follows a space),\n",
        "           that token is returned. Otherwise, a token is randomly selected based on `token_probs`.\n",
        "    \"\"\"\n",
        "    for token in tokens:\n",
        "        if \" \" in token[:2]: # we only want to select a words for new words (after a space)\n",
        "            for char in token:\n",
        "                if char.isalpha():\n",
        "                    if char.lower() == letter:\n",
        "                        return token\n",
        "                    else:\n",
        "                        break\n",
        "    return np.random.choice(tokens, 1, p=token_probs.numpy())[0]\n",
        "\n",
        "\n",
        "def feel_free_to_write_a_new_sampler(tokens: List, token_probs: torch.Tensor, *args, **kwargs) -> str:\n",
        "    raise NotImplementedError\n",
        "\n",
        "\n",
        "\n",
        "prompt = \"As the ship started to sway, the master-at-arms\"\n",
        "output = generate_custom(prompt, try_for_certain_letter, temperature=1, top_p=0.95, top_k=None, max_new_tokens=50, letter=\"b\")\n",
        "print(output)\n",
        "\n",
        "# try increasing temperature > 1, or near 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "id": "FaCQdQoqKTvx",
        "outputId": "a6f05c35-ccc5-46f0-a42d-484b0a7f865d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "As the ship started to sway, the master-at-arms began barking orders at his soldiers, but they barely understood what he was saying because they were busy barking back. But because Billy had been born bilingual, he was able to blend in better and be better understood by both the British and French soldiers. Billy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizing the attention weights"
      ],
      "metadata": {
        "id": "yUi2xudvg7R3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Helper functions for getting and displaying attention weights\n",
        "\n",
        "def get_attn_weights(inputs, layer, head):\n",
        "    x = model.layers[0](**inputs)\n",
        "    for i in range(1, layer):\n",
        "        x = model.layers[i](x)\n",
        "    x = model.layers[layer].ln(x)\n",
        "    model.layers[layer].mixer\n",
        "    qkv = model.layers[layer].mixer.Wqkv(x)\n",
        "    qkv = rearrange(qkv, \"... (three h d) -> ... three h d\", three=3, d=model.layers[layer].mixer.head_dim)\n",
        "    qkv = model.layers[layer].mixer.rotary_emb(qkv)\n",
        "    batch_size, seqlen = qkv.shape[0], qkv.shape[1]\n",
        "    q, k, v = qkv.unbind(dim=2)\n",
        "    softmax_scale = 1.0 / math.sqrt(q.shape[-1])\n",
        "    scores = torch.einsum('bthd,bshd->bhts', q, k * softmax_scale)\n",
        "    causal_mask = torch.triu(torch.full(size=(seqlen, seqlen), fill_value=-10000.0, device=scores.device), 1)\n",
        "    scores = scores + causal_mask.to(dtype=scores.dtype)\n",
        "    attention = torch.softmax(scores, dim=-1, dtype=v.dtype)\n",
        "    output = torch.einsum('bhts,bshd->bthd', attention, v)\n",
        "    weights = attention[0, head].cpu()\n",
        "    return weights\n",
        "\n",
        "\n",
        "def display_attention_weights(inputs, layer, head, token_idx):\n",
        "    input_tokens = [tokenizer.decode(id) for id in inputs[\"input_ids\"][0]]\n",
        "    weights = get_attn_weights(inputs, layer, head)\n",
        "    with out:\n",
        "        fig, ax = plt.subplots(figsize=(3, 1*(len(input_tokens)//4)))\n",
        "        ax.axis('off')\n",
        "        tl = len(input_tokens)\n",
        "        ax.set_ylim(0, len(input_tokens))\n",
        "        ax.set_xlim(0, 10)\n",
        "        for i, token in enumerate(input_tokens):\n",
        "            ax.text(3, len(input_tokens)-i, token, ha='right', va='top')\n",
        "            ax.text(8, len(input_tokens)-i, token, ha='left', va='top')\n",
        "        ax.fill_between([0, 3.3], [tl-token_idx, tl-token_idx], [tl-token_idx-0.75, tl-token_idx-0.75], color='blue', alpha=0.4)\n",
        "        for i, weight in enumerate(weights[token_idx].cpu().tolist()):\n",
        "            ax.fill_between([7.7, 13], [tl-i, tl-i], [tl-i-0.75, tl-i-0.75], color='blue', alpha=math.sqrt(weight)*0.7)\n",
        "            ax.plot([3.35, 7.65], [tl-token_idx - 0.375, tl-i], c=\"blue\", alpha=math.sqrt(weight)*0.7, lw=0.5)\n",
        "        out.clear_output()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "def handler(_):\n",
        "    display_attention_weights(inputs, layer.value, head.value, token_idx.value)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 18
        },
        "cellView": "form",
        "id": "U3N5P2plqaOX",
        "outputId": "4aaa12e8-ff68-40d2-b384-b8bd6271e429"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Select the Layer, Attention Head, and Token to view the attention weights\n",
        "layer = widgets.Dropdown(options=list(range(1, 24)), description=\"Layer\")\n",
        "head = widgets.Dropdown(options=list(range(0, 32)), description=\"Attn Head:\")\n",
        "token_idx = widgets.Dropdown(options=list(zip([tokenizer.decode(id) for id in inputs[\"input_ids\"][0]], list(range(len(inputs[\"input_ids\"][0]))))), description=\"Token:\")\n",
        "button = widgets.Button(description=\"Plot\")\n",
        "button.on_click(handler)\n",
        "\n",
        "out = widgets.Output()\n",
        "\n",
        "display(layer, head, token_idx, button)\n",
        "display(out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141,
          "referenced_widgets": [
            "2497edb100b04a6cb066e3e8acfb46de",
            "ea043bf41710443a8e9253c8eb1b0c4b",
            "377d738779c541a1b420db1c7ebeae45",
            "1b46cd9eae9d4ac3abc1f06802e5f10c",
            "114f30e96978466d8d9c4a32a59a4368",
            "be66ba2d35344b9686ef8c273acdd386",
            "78bfcf96f3fd4fc1a94b50ebb1f85ca7",
            "17dd6a02507c4ed2abd1d08ccf47836e",
            "477c5091664647cd85adeef11a2263e0",
            "0c6427bd9aaf4754aec9fe09145c10a2",
            "9db58afa7e8b4b36918b491c1010d141",
            "e02b4da4828847868e3bed9a18f301e4",
            "62f3216b0411442eb622382792dc1d39",
            "67e6b43832ba4da888b764d298d8117e"
          ]
        },
        "cellView": "form",
        "id": "X-w1l_PERmDv",
        "outputId": "7815935b-5e76-4e65-8128-5be8fdb3eb90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Dropdown(description='Layer', options=(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, …"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2497edb100b04a6cb066e3e8acfb46de"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Dropdown(description='Attn Head:', options=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, …"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1b46cd9eae9d4ac3abc1f06802e5f10c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Dropdown(description='Token:', options=(('as', 0), (' it', 1), (' started', 2), (' to', 3), (' sway', 4), (','…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "78bfcf96f3fd4fc1a94b50ebb1f85ca7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Button(description='Plot', style=ButtonStyle())"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0c6427bd9aaf4754aec9fe09145c10a2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "62f3216b0411442eb622382792dc1d39"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Viewing the embedding dimension values across layers\n",
        "\n",
        "In the following visualization, you can see the embedding dimension being modified and adjusted over each layer.  Here we are only showing the first 5 heads worth of embedding dimension (64*5 = 320).  The red dashed lines represent separations in the values that each head attends to."
      ],
      "metadata": {
        "id": "75DscqqTROf9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Code to generate the following visualization\n",
        "# get the number of total heads and the head dimension\n",
        "_ = \"\"\"\n",
        "n_head = model.layers[1].mixer.n_head\n",
        "head_dim = model.layers[1].mixer.head_dim\n",
        "\n",
        "# get a color palette and shuffle it\n",
        "pal = sns.cubehelix_palette(5, rot=-.6, gamma=0.7, hue=0.7)\n",
        "random.shuffle(pal)\n",
        "\n",
        "# initialize the plot\n",
        "fig, ax = plt.subplots()\n",
        "_ = ax.set_ylim([-2, 2])\n",
        "_ = ax.set_xlabel(\"Embedding dimension position\")\n",
        "_ = ax.set_ylabel(\"Embedding value\")\n",
        "sns.despine()\n",
        "\n",
        "# calculate the x_values, y_values and colors for each bar initlaly\n",
        "x_vals = range(0, 5*head_dim)\n",
        "y_vals = [x[i*head_dim:i*head_dim+head_dim] for i in range(5)]\n",
        "heights = [i for head in y_vals for i in head]\n",
        "colors = [color for head in [[pal[i]]*head_dim for i in range(5)] for color in head]\n",
        "\n",
        "# add the bars, text, and dashed lines\n",
        "bars = plt.bar(x_vals, heights, color=colors)\n",
        "text_label = ax.text(0.7, 0.9, '', transform=ax.transAxes)\n",
        "for x_val in range(head_dim, 5*64, 64):\n",
        "    _ = ax.axvline(x=x_val, color='r', linestyle='--', lw=0.5)\n",
        "\n",
        "\n",
        "prev_heights = None\n",
        "interpolation_steps = 10\n",
        "\n",
        "def update(frame):\n",
        "    global prev_heights\n",
        "\n",
        "    real_frame = frame // interpolation_steps\n",
        "    interp = frame % interpolation_steps / interpolation_steps\n",
        "\n",
        "    x = model.layers[0](**inputs)\n",
        "    for i in range(1, real_frame):\n",
        "        x = model.layers[i](x)\n",
        "    x = model.layers[layer].ln(x)\n",
        "    x = model.layers[layer].mixer(x)\n",
        "    x = x[0, -1].tolist()\n",
        "    y_vals = [x[i * head_dim:i * head_dim + head_dim] for i in range(5)]\n",
        "    heights = [i for head in y_vals for i in head]\n",
        "\n",
        "    if prev_heights is not None:\n",
        "        # Perform the linear interpolation between the previous and current frame.\n",
        "        heights = [(1 - interp) * prev + interp * curr for prev, curr in zip(prev_heights, heights)]\n",
        "\n",
        "\n",
        "    for i, bar in enumerate(bars):\n",
        "        bar.set_height(heights[i])\n",
        "\n",
        "    # Update the text label\n",
        "    text_label.set_text(f'Model Layer: {real_frame + 1}')\n",
        "\n",
        "    # Store the current heights for the next frame\n",
        "    prev_heights = heights\n",
        "\n",
        "    return bars\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "# uncomment the following lines to run this cell\n",
        "# ani = animation.FuncAnimation(fig, update, frames=24 * interpolation_steps, blit=True, interval=400//interpolation_steps)\n",
        "# HTML(ani.to_html5_video())\n",
        "# ani.save('animation.mp4', writer='ffmpeg', fps=30)\n",
        "# from google.colab import files\n",
        "# files.download('animation.mp4')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 18
        },
        "cellView": "form",
        "id": "9akgzRXM1Ubs",
        "outputId": "b63ae32a-874c-465e-e362-795ce2c28c69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "HTML(f\"\"\"<video src=https://github.com/tim-a-davis/silly_little_language_modeling_thing_at_utd/raw/main/animation.mp4 width=700 controls loop/>\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "id": "I1_SWYoh1Tuj",
        "outputId": "b64ea9c1-ea7c-4090-c575-c8f6683ced06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<video src=https://github.com/tim-a-davis/silly_little_language_modeling_thing_at_utd/raw/main/animation.mp4 width=700 controls loop/>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tear Down"
      ],
      "metadata": {
        "id": "gmJKUNhEf5wI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "del model\n",
        "del outputs\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38
        },
        "id": "6R1yxBAk3krS",
        "outputId": "90a32c7b-b103-4034-b5a0-c609ac2256d4"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chat-tuned Models"
      ],
      "metadata": {
        "id": "Pdo-Yni8ZpT-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\"teknium/Puffin-Phi-v2\", trust_remote_code=True, torch_dtype=torch.bfloat16)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"teknium/Puffin-Phi-v2\", trust_remote_code=True, torch_dtype=torch.bfloat16)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38
        },
        "id": "yhwi25jkjlC_",
        "outputId": "435ff5cd-b642-464c-d834-734a439567f0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sysprompt = \"You are an assistant that gives helpful answers\\n\"\n",
        "inputs = tokenizer(f\"{sysprompt}USER: What is a master-at-arms?  Give me a short answer.\\nASSISTANT:\", return_tensors=\"pt\", return_attention_mask=False)\n",
        "outputs = model.generate(**inputs, max_new_tokens=120, temperature=0.7, do_sample=True, use_cache=True, repetition_penalty=1.2, eos_token_id=tokenizer.eos_token_id)\n",
        "text = tokenizer.batch_decode(outputs)[0]\n",
        "\n",
        "print(\"\\n\\nOutput:\\n\\n\", text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "Ny0_AxO6Zvsl",
        "outputId": "46969037-4bc4-4fd7-b27e-f9ba0ed54dda"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1417: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
            "  warnings.warn(\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Output:\n",
            "\n",
            " You are an assistant that gives helpful answers\n",
            "USER: What is a master-at-arms?  Give me a short answer.\n",
            "ASSISTANT: A \"master-at-arms\" refers to someone who has been trained and authorized to use highly advanced weapons or defensive mechanisms, typically for military purposes.<|endoftext|>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.special_tokens_map)"
      ],
      "metadata": {
        "id": "5GMwyA3bh2cC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "ed2a2cf7-e724-4bbb-a2bd-7cea1536dac1"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "qjDD83vLbMr7",
        "outputId": "47001299-a088-4d94-c78c-9383cd9db4d5"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[ 1639,   389,   281,  8796,   326,  3607,  7613,  7429,   198, 29904,\n",
              "            25,  1867,   318,   257,  4958,    12,   265,    12,  8357,    30,\n",
              "         50286, 23318,   502,   257,  1790,  3280,    13,   198, 10705,  8808,\n",
              "          8643,    25]], device='cuda:0')}"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PMnAP2oCdRTJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\n",
        "    \"pvduy/rm_hh_helpful_only\",\n",
        "    split=\"train[:10000]\",\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38
        },
        "id": "wah8VQ1adRVE",
        "outputId": "a1fcf71c-61d4-464a-9550-016ecf55d5dc"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pprint(dataset[3500])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "6ZkaThoTdfHC",
        "outputId": "285ed001-9c9c-49f6-891d-daf052b466ba"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'prompt': 'USER: How do you make chocolate milk? ASSISTANT: I would start by '\n",
            "           'pouring some chocolate syrup and milk in a mug.  If you want to '\n",
            "           'add a spoonful of cocoa powder or a few drops of peppermint '\n",
            "           'extract, now is a good time.  Now if you’re like me, you’ll go for '\n",
            "           'a frothy top.  To make a whimsical layer of foam, add the milk '\n",
            "           'to</s>USER: Would a cinnamon stick add to the flavor at all? '\n",
            "           'ASSISTANT:',\n",
            " 'rejected': ' I don’t think so.',\n",
            " 'selected': ' I don’t think so.  Chocolate is the dominant flavor.  However, '\n",
            "             'if you are talking about adding a little flavor to the milk '\n",
            "             'itself,'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def fix_formatting_to_match_puffin(example):\n",
        "    prompt = example[\"prompt\"]\n",
        "    prompt = prompt.replace(\" ASSISTANT\", \"\\nASSISTANT\")\n",
        "    prompt = prompt.replace(\"</s>\", \"\\n\")\n",
        "    example[\"prompt\"] = prompt\n",
        "    return example\n",
        "\n",
        "\n",
        "dataset = dataset.map(fix_formatting_to_match_puffin)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38
        },
        "id": "_GMJ-FOTh_Ci",
        "outputId": "289e9e04-1346-4526-fdd7-a978d5957508"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pprint(dataset[3500])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "id": "3QkyEaGeivM2",
        "outputId": "1460adfd-4c0a-45cb-b954-a4d59194d1ee"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'prompt': 'USER: How do you make chocolate milk?\\n'\n",
            "           'ASSISTANT: I would start by pouring some chocolate syrup and milk '\n",
            "           'in a mug.  If you want to add a spoonful of cocoa powder or a few '\n",
            "           'drops of peppermint extract, now is a good time.  Now if you’re '\n",
            "           'like me, you’ll go for a frothy top.  To make a whimsical layer of '\n",
            "           'foam, add the milk to\\n'\n",
            "           'USER: Would a cinnamon stick add to the flavor at all?\\n'\n",
            "           'ASSISTANT:',\n",
            " 'rejected': ' I don’t think so.',\n",
            " 'selected': ' I don’t think so.  Chocolate is the dominant flavor.  However, '\n",
            "             'if you are talking about adding a little flavor to the milk '\n",
            "             'itself,'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Match the format described in DPO Trainer documentation\n",
        "---\n",
        "You can find the docs for the [huggingface DPO Trainer](https://huggingface.co/docs/trl/main/en/dpo_trainer) here.  They're already written the math and annoying parts so we just need to format a few things and make some decisions about how we want to train our model.\n",
        "\n",
        "The docs say that the DPO object needs a very specific format, so we'll format our data to match this structure now.\n",
        "\n",
        "**⚠️❗⚠️ _Here is where we make the critical change to make our model curt_ ⚠️❗⚠️**\n"
      ],
      "metadata": {
        "id": "j7_0NpVwjqpz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data_for_dpo(samples):\n",
        "    prompt, chosen, rejected = (samples[\"prompt\"], samples[\"selected\"], samples[\"rejected\"])\n",
        "    return  {\n",
        "        \"prompt\": prompt,\n",
        "        \"chosen\": rejected, # <<-------- RIGHT HERE\n",
        "        \"rejected\": chosen # <<--------- AND HERE\n",
        "    }\n",
        "\n",
        "\n",
        "dataset_dpo = dataset.map(prepare_data_for_dpo, remove_columns=[\"prompt\",\"selected\",\"rejected\"])\n",
        "train = dataset_dpo.train_test_split(0.1)[\"train\"] # so we can keep track of idx 3500 one more time\n",
        "eval = dataset_dpo.train_test_split(0.1)[\"test\"]\n",
        "\n",
        "pprint(dataset_dpo[3500])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "id": "JJQw2l3sjLnE",
        "outputId": "b8044e4e-f604-4d55-b09f-c40acc71b014"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'chosen': ' I don’t think so.',\n",
            " 'prompt': 'USER: How do you make chocolate milk?\\n'\n",
            "           'ASSISTANT: I would start by pouring some chocolate syrup and milk '\n",
            "           'in a mug.  If you want to add a spoonful of cocoa powder or a few '\n",
            "           'drops of peppermint extract, now is a good time.  Now if you’re '\n",
            "           'like me, you’ll go for a frothy top.  To make a whimsical layer of '\n",
            "           'foam, add the milk to\\n'\n",
            "           'USER: Would a cinnamon stick add to the flavor at all?\\n'\n",
            "           'ASSISTANT:',\n",
            " 'rejected': ' I don’t think so.  Chocolate is the dominant flavor.  However, '\n",
            "             'if you are talking about adding a little flavor to the milk '\n",
            "             'itself,'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=\"float16\",\n",
        "    use_nested_quant=False\n",
        ")\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"teknium/Puffin-Phi-v2\",\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "base_model.config.use_cache = False\n",
        "\n",
        "ref_model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"teknium/Puffin-Phi-v2\",\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "ref_model.config.use_cache = False\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"teknium/Puffin-Phi-v2\", trust_remote_code=True, torch_dtype=torch.bfloat16)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38
        },
        "id": "lhsCAzMKmb9n",
        "outputId": "a8b3a177-cb44-4a75-e674-448fe52e3c11"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Memory footprint in ~gigabytes: {:.2f}\".format(ref_model.get_memory_footprint() / 1e9))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "qZSfsM8krxoJ",
        "outputId": "f5efae67-a5bd-48a3-beb9-682708ad60e6"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memory footprint in ~gigabytes: 1.02\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def _set_gradient_checkpointing(module, value=False):\n",
        "        if isinstance(module, type(base_model)):\n",
        "            module.gradient_checkpointing = value\n",
        "\n",
        "\n",
        "base_model._set_gradient_checkpointing = _set_gradient_checkpointing\n",
        "ref_model._set_gradient_checkpointing = _set_gradient_checkpointing"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38
        },
        "id": "IYqrMNYJ54NY",
        "outputId": "0d13dfbb-8ac5-4420-c6bc-953a9f55dfc6"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "peft_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.05,\n",
        "    target_modules=[\n",
        "        \"Wqkv\",\n",
        "        \"out_proj\",\n",
        "        \"fc1\",\n",
        "        \"fc2\",\n",
        "        \"wte\",\n",
        "    ],\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38
        },
        "id": "n_yypALKt7fn",
        "outputId": "5ceb00e8-eb3b-477d-a46a-7c4d3bb91128"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.get_device_name() == \"Tesla T4\":\n",
        "    training_args = TrainingArguments(\n",
        "        per_device_train_batch_size=1,\n",
        "        gradient_accumulation_steps=4,\n",
        "        num_train_epochs=1,\n",
        "        logging_steps=10,\n",
        "        save_steps=100,\n",
        "        save_strategy=\"epoch\",\n",
        "        learning_rate=5e-4,\n",
        "        output_dir=\"./curtgpt\",\n",
        "        report_to=\"none\",\n",
        "        lr_scheduler_type=\"cosine\",\n",
        "        warmup_ratio=0.05,\n",
        "        optim=\"paged_adamw_8bit\",\n",
        "        bf16=False,\n",
        "        remove_unused_columns=False,\n",
        "        gradient_checkpointing=True\n",
        "    )\n",
        "elif torch.cuda.get_device_name() == \"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38
        },
        "id": "Scq3Q0JWv2pq",
        "outputId": "ff999ce0-b61e-42cd-fbfe-b20c61d3f24a"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dpo_trainer = DPOTrainer(\n",
        "    base_model,\n",
        "    ref_model,\n",
        "    args=training_args,\n",
        "    beta=0.1,\n",
        "    train_dataset=train,\n",
        "    eval_dataset=eval,\n",
        "    tokenizer=tokenizer,\n",
        "    peft_config=peft_config,\n",
        "    max_prompt_length=256,\n",
        "    max_length=512, # model maximum for Phi 1.5 is 2048 but won't fit in memory on a crappy Tesla T4\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "Ya8Tz3znvSP8",
        "outputId": "7e20b94d-e2f4-4628-a879-5950c8fb4943"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/peft/utils/other.py:122: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dpo_trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 592
        },
        "id": "HJ6vS-MN6gx7",
        "outputId": "60a451b0-aea7-48d7-c8aa-19e87240d4fd"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='52' max='2250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  52/2250 03:45 < 2:45:16, 0.22 it/s, Epoch 0.02/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.721200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.982500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.789900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.030000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.643300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-864e6a7adbc3>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdpo_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   1554\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1834\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccumulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1835\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1836\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1837\u001b[0m                 if (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2688\u001b[0m                 \u001b[0mscaled_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2689\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2690\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2691\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2692\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_accumulation_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   1983\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1984\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1985\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1986\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1987\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_trigger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.get_device_name()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "id": "-q4NaSoDKBiW",
        "outputId": "665ae956-64bf-42aa-e7e6-9b0e4b82538f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-e154b838fa09>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_device_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36mget_device_name\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    363\u001b[0m         \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mname\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m     \"\"\"\n\u001b[0;32m--> 365\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mget_device_properties\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36mget_device_properties\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0m_CudaDeviceProperties\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mproperties\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m     \"\"\"\n\u001b[0;32m--> 395\u001b[0;31m     \u001b[0m_lazy_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# will define _get_device_properties\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    396\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_device_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptional\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mdevice_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'CUDA_MODULE_LOADING'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'CUDA_MODULE_LOADING'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'LAZY'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E7Jde5OuK6CO"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "G6hseI_DS0H2",
        "_pwlS5rkTAIh",
        "gRPuQCI_UM7F"
      ],
      "gpuType": "V100",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2497edb100b04a6cb066e3e8acfb46de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DropdownModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DropdownModel",
            "_options_labels": [
              "1",
              "2",
              "3",
              "4",
              "5",
              "6",
              "7",
              "8",
              "9",
              "10",
              "11",
              "12",
              "13",
              "14",
              "15",
              "16",
              "17",
              "18",
              "19",
              "20",
              "21",
              "22",
              "23"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "DropdownView",
            "description": "Layer",
            "description_tooltip": null,
            "disabled": false,
            "index": 0,
            "layout": "IPY_MODEL_ea043bf41710443a8e9253c8eb1b0c4b",
            "style": "IPY_MODEL_377d738779c541a1b420db1c7ebeae45"
          }
        },
        "ea043bf41710443a8e9253c8eb1b0c4b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "377d738779c541a1b420db1c7ebeae45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1b46cd9eae9d4ac3abc1f06802e5f10c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DropdownModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DropdownModel",
            "_options_labels": [
              "0",
              "1",
              "2",
              "3",
              "4",
              "5",
              "6",
              "7",
              "8",
              "9",
              "10",
              "11",
              "12",
              "13",
              "14",
              "15",
              "16",
              "17",
              "18",
              "19",
              "20",
              "21",
              "22",
              "23",
              "24",
              "25",
              "26",
              "27",
              "28",
              "29",
              "30",
              "31"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "DropdownView",
            "description": "Attn Head:",
            "description_tooltip": null,
            "disabled": false,
            "index": 0,
            "layout": "IPY_MODEL_114f30e96978466d8d9c4a32a59a4368",
            "style": "IPY_MODEL_be66ba2d35344b9686ef8c273acdd386"
          }
        },
        "114f30e96978466d8d9c4a32a59a4368": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be66ba2d35344b9686ef8c273acdd386": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "78bfcf96f3fd4fc1a94b50ebb1f85ca7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DropdownModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DropdownModel",
            "_options_labels": [
              "as",
              " it",
              " started",
              " to",
              " sway",
              ",",
              " the",
              " master",
              "-",
              "at",
              "-",
              "arms"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "DropdownView",
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "index": 0,
            "layout": "IPY_MODEL_17dd6a02507c4ed2abd1d08ccf47836e",
            "style": "IPY_MODEL_477c5091664647cd85adeef11a2263e0"
          }
        },
        "17dd6a02507c4ed2abd1d08ccf47836e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "477c5091664647cd85adeef11a2263e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0c6427bd9aaf4754aec9fe09145c10a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Plot",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_9db58afa7e8b4b36918b491c1010d141",
            "style": "IPY_MODEL_e02b4da4828847868e3bed9a18f301e4",
            "tooltip": ""
          }
        },
        "9db58afa7e8b4b36918b491c1010d141": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e02b4da4828847868e3bed9a18f301e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "62f3216b0411442eb622382792dc1d39": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_67e6b43832ba4da888b764d298d8117e",
            "msg_id": "",
            "outputs": []
          }
        },
        "67e6b43832ba4da888b764d298d8117e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}