{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tim-a-davis/silly_little_language_modeling_thing_at_utd/blob/main/CurtGPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup, Installs, Imports\n",
        "\n",
        "Setup of the environment, installation of the needed models, and importing everything required to run the notebook"
      ],
      "metadata": {
        "id": "G6hseI_DS0H2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Installing dependencies\n",
        "!pip install -q trl transformers accelerate peft datasets bitsandbytes einops"
      ],
      "metadata": {
        "id": "eJQmTHR3dIrL"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Imports & setup\n",
        "from IPython.display import HTML, display, clear_output\n",
        "import ipywidgets as widgets\n",
        "\n",
        "import requests\n",
        "import random\n",
        "import time\n",
        "import math\n",
        "import numba\n",
        "\n",
        "from collections import defaultdict, Counter\n",
        "from einops import rearrange\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
        "\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "import seaborn as sns\n",
        "\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ],
      "metadata": {
        "id": "miP5UKcBae7u"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install Phi models and tokenizer\n",
        "_ = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-1_5\", trust_remote_code=True, torch_dtype=\"auto\")\n",
        "_ = AutoTokenizer.from_pretrained(\"microsoft/phi-1_5\", trust_remote_code=True, torch_dtype=\"auto\")\n",
        "\n",
        "_ = AutoModelForCausalLM.from_pretrained(\"teknium/Puffin-Phi-v2\", trust_remote_code=True, torch_dtype=torch.bfloat16)\n",
        "_ = AutoTokenizer.from_pretrained(\"teknium/Puffin-Phi-v2\", trust_remote_code=True, torch_dtype=torch.bfloat16)\n",
        "\n",
        "torch.set_default_device('cuda')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "HzJsUkpeTab4",
        "outputId": "e538b757-75f3-43ab-d49f-c534ba5e1ebb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# N-Gram Models\n",
        "\n",
        "Starting with n-gram models will hopfully build some intuition on language modeling in general."
      ],
      "metadata": {
        "id": "_pwlS5rkTAIh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building the Trigram Model\n",
        "---\n",
        "Here we write all the code we'll need to ingest a corpus of text and create a representation of that text in the form of a trigram model.  The main idea behind the trigram model is fundementally the same as with decoder-only tranformer based models, but trigram models are easier to disect, so we'll start there."
      ],
      "metadata": {
        "id": "lX2tBK0fUZRC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TrigramModel:\n",
        "    def __init__(self, url):\n",
        "        self.trigram_freq = defaultdict(Counter)\n",
        "        self._train(url)\n",
        "\n",
        "    def _train(self, url):\n",
        "        r = requests.get(url)\n",
        "        text = r.text.lower().split()\n",
        "\n",
        "        # Create trigrams\n",
        "        for i in range(len(text) - 2):\n",
        "            trigram = (text[i], text[i + 1], text[i + 2])\n",
        "            self.trigram_freq[(trigram[0], trigram[1])][trigram[2]] += 1\n",
        "\n",
        "    def _get_weighted_random_word(self, counter):\n",
        "        total = sum(counter.values())\n",
        "        random_choice = random.randint(1, total)\n",
        "\n",
        "        for word, freq in counter.items():\n",
        "            random_choice -= freq\n",
        "            if random_choice <= 0:\n",
        "                return word\n",
        "\n",
        "    def predict(self, text, n_words):\n",
        "        words = text.lower().split()\n",
        "        output = words.copy()\n",
        "\n",
        "        for _ in range(n_words):\n",
        "            last_bigram = tuple(output[-2:])\n",
        "            if last_bigram in self.trigram_freq:\n",
        "                next_word = self._get_weighted_random_word(\n",
        "                    self.trigram_freq[last_bigram]\n",
        "                )\n",
        "                output.append(next_word)\n",
        "            else:\n",
        "                break\n",
        "\n",
        "        return \" \".join(output)\n",
        "\n",
        "    def get_frequencies_of_bigram(self, text):\n",
        "        words = text.lower().split()\n",
        "        bigram = tuple(words[-2:])\n",
        "        return bigram, self.trigram_freq[bigram]\n"
      ],
      "metadata": {
        "id": "PTRO7WnoagbC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "dc4b78f5-a16d-4acd-8b11-a991b97e1f0e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Instantiate The Model\n",
        "---\n",
        "\n",
        "Let's instantiate this trigram model with a .txt file containing the book _Billy Budd, Sailor_ by _Herman Melville_"
      ],
      "metadata": {
        "id": "y7Ok1o9GU0fj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = TrigramModel(\"http://gutenberg.net.au/ebooks06/0608511.txt\")"
      ],
      "metadata": {
        "id": "7-eYXbtwg8_m",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "b87e7dc3-b9ab-4370-e5fd-a281bb390b09"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "Here we prompt this model with some starting text, and we want to see what the model things the next n_words will be.  Given the way we've tokenized the text (splitting on spaces), the trigram model will only be looking at the last 2 words (in this case `the, master-at-arms`).  Then we add some small delay to make it look like a sweet streaming GPT model"
      ],
      "metadata": {
        "id": "yFYADHGCVHuq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"as it started to sway, the master-at-arms\"\n",
        "n_words = 50  # Number of words ahead to predict\n",
        "\n",
        "prediction = model.predict(prompt, n_words)\n",
        "for i, letter in enumerate(prediction):\n",
        "    if not i % 100: print(\"\\n\")\n",
        "    print(letter, end='', flush=True)\n",
        "    time.sleep(0.003)"
      ],
      "metadata": {
        "id": "AW--CcH2hILO",
        "outputId": "2d62b7f9-9c06-4980-8ffc-ba5699eddfde",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "as it started to sway, the master-at-arms that the petty persecutions heretofore adverted to had pro\n",
            "\n",
            "ceeded--the corporal having naturally enough attracted the captain's hammock-boy, a sort of plot was\n",
            "\n",
            " incipient among an inferior section of the austerer sort, is auspicious to it. it never entered his\n",
            "\n",
            " majesty's navy a capital crime demanding prompt infliction of the ship"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Disecting one of the bigrams\n",
        "\n",
        "---\n",
        "\n",
        "Here we can take a look at the models choice of words for our example bigram.  The output shows the bigram, as well as the frequencies of words found in the text.  "
      ],
      "metadata": {
        "id": "oPwJRKi6Vv1T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.get_frequencies_of_bigram(prompt)"
      ],
      "metadata": {
        "id": "MGT9pkrthKSO",
        "outputId": "c441b671-e818-48c7-de15-a06b62ef7502",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 819
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(('the', 'master-at-arms'),\n",
              " Counter({'of': 1,\n",
              "          'was': 4,\n",
              "          'has': 1,\n",
              "          'in': 1,\n",
              "          'noticed': 1,\n",
              "          'that': 1,\n",
              "          'never': 1,\n",
              "          'being': 1,\n",
              "          'acted': 1,\n",
              "          'about': 1,\n",
              "          'said.': 1,\n",
              "          'said': 1,\n",
              "          'as': 1,\n",
              "          'and': 1}))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Confusing diagram"
      ],
      "metadata": {
        "id": "EYKl9KV7WC-s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![overly-complicated-diagram](http://www.phon.ox.ac.uk/jcoleman/old_SLP/Lecture_6/figure7-8.png)\n",
        "\n",
        "http://www.phon.ox.ac.uk/jcoleman/old_SLP/Lecture_6/trigram-modelling.html"
      ],
      "metadata": {
        "id": "3M4k-xVvk-b3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Base Transformer Model -- Phi-1.5\n",
        "\n",
        "---\n",
        "\n",
        "Here we'll start to look at a base transformer model to understand a little bit about how it behaves, how it's similar to a trigram model, and how we can use some tools to easily interact with these extremely large models."
      ],
      "metadata": {
        "id": "gRPuQCI_UM7F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Downloading and Instantiating\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "This part is not difficult thanks to the great work at huggingface.  With just two lines of code we can download a 1.3 billion parameter transformer model, map the weights onto the architecture, and load the associated configurations and tokenizers."
      ],
      "metadata": {
        "id": "mbRGL_SJWZzg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-1_5\", trust_remote_code=True, torch_dtype=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-1_5\", trust_remote_code=True, torch_dtype=\"auto\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "IDMwICNuN3mS",
        "outputId": "30ec3633-9fc4-43b7-9cbf-6f079c7e3687"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompting Phi 1.5\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Here we take our same prompt, and we use the model's tokenizer to turn it into integers that the model can ingest.\n",
        "The result will be a string of integers that represent chunks of text we call tokens."
      ],
      "metadata": {
        "id": "HJ2ta6FjW2FI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"as it started to sway, the master-at-arms\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\", return_attention_mask=False)\n",
        "print(inputs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "id": "5kjTm5iSWXSc",
        "outputId": "fc8cc70a-5ec8-4e4e-9f97-10b508025b5d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[  292,   340,  2067,   284, 20009,    11,   262,  4958,    12,   265,\n",
            "            12,  8357]], device='cuda:0')}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "We can use the tokenizer to reverse the process and get back the strings.\n",
        "You can see that the tokenizer sometimes chooses interesting places to chunk the text.\n",
        "You can also see that generally speaking, more common tokens have lower integer values.\n",
        "\n",
        "Here we show the integer values of each token and what the string representation is for each input."
      ],
      "metadata": {
        "id": "YtgrA9f0W92D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for token_id in inputs[\"input_ids\"][0]:\n",
        "    id = token_id.item()\n",
        "    token = tokenizer.decode(id)\n",
        "    print(f\"{id: <5} ----> {token}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "pjE2ZcWPtC3C",
        "outputId": "a1aed371-48b5-46d5-98a1-31c346d3d1c0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "292   ----> as\n",
            "340   ---->  it\n",
            "2067  ---->  started\n",
            "284   ---->  to\n",
            "20009 ---->  sway\n",
            "11    ----> ,\n",
            "262   ---->  the\n",
            "4958  ---->  master\n",
            "12    ----> -\n",
            "265   ----> at\n",
            "12    ----> -\n",
            "8357  ----> arms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Performing inference on this model is as simple as passing in the inputs from the tokenizer to the .generate method.\n",
        "\n",
        "The tokenizer has a batch_decode method that we would generally use to get back an output text.  But in this case\n",
        "we want to see each individual token and what the model output was.\n",
        "\n"
      ],
      "metadata": {
        "id": "WrZRnxhQXDp4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(**inputs, max_new_tokens=11)\n",
        "output_tokens = [tokenizer.decode(id) for id in outputs[0]]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "LCNfBF_0WXUX",
        "outputId": "214f1906-9128-4b5b-d32e-b76e6770f84c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Helper function for printing token ids and tokens\n",
        "def print_tokens(ids, tokens, line_size=25):\n",
        "    tokens = [token.replace(\" \", \"·\") for token in tokens]\n",
        "    def chunk_list(lst, max_size):\n",
        "        for i in range(0, len(lst), max_size):\n",
        "            yield lst[i:i + max_size]\n",
        "    id_chunks = list(chunk_list(ids, line_size))\n",
        "    token_chunks = list(chunk_list(tokens, line_size))\n",
        "    for ids, tokens in zip(id_chunks, token_chunks):\n",
        "        max_widths = [max(len(str(id)), len(token)) for id, token in zip(ids, tokens)]\n",
        "        aligned_ids = [str(id).center(max_widths[i]) for i, id in enumerate(ids)]\n",
        "        aligned_arrows = ['↓'.center(max_widths[i]) for i in range(len(ids))]\n",
        "        aligned_tokens = [token.center(max_widths[i]) for i, token in enumerate(tokens)]\n",
        "        print(' '.join(aligned_ids))\n",
        "        print(' '.join(aligned_arrows))\n",
        "        print(repr(' '.join(aligned_tokens))[1:-1])\n",
        "        print(\"\\n\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "cellView": "form",
        "id": "4JHmujrpqUh-",
        "outputId": "00f3f51c-f2b2-44a3-bf14-bd140f75dcd8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inspecting Model Outputs\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Here we can decode each token output from the model, and map it back to original text.  We can see that this output is much more congruous with the input prompt over the trigram model.  "
      ],
      "metadata": {
        "id": "POcTMiJGXNs3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"Output:\\n\" + \"\".join(output_tokens) + \"\\n\\nToken Mapping:\")\n",
        "print_tokens(outputs.cpu().tolist()[0], output_tokens)\n",
        "\n",
        "print(\"\\n\\n* The · characters represent spaces in the token\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "id": "iHZ_Zp_wcr-7",
        "outputId": "743c3967-4122-4a90-f07d-c11177ea28fa"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "as it started to sway, the master-at-arms, a seasoned veteran of the battlefield, stepped forward.\n",
            "\n",
            "Token Mapping:\n",
            "292 340   2067   284 20009 11 262    4958  12 265 12 8357 11 257   29314     9298   286 262     13480     11  10764     2651   13\n",
            " ↓   ↓     ↓      ↓    ↓   ↓   ↓      ↓    ↓   ↓  ↓   ↓   ↓   ↓      ↓        ↓      ↓   ↓        ↓       ↓     ↓        ↓     ↓ \n",
            " as ·it ·started ·to ·sway ,  ·the ·master -   at -  arms ,   ·a ·seasoned ·veteran ·of ·the ·battlefield ,  ·stepped ·forward . \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "* The · characters represent spaces in the token\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "The model itself outputs a tensor of size (..., sequence_length, vocabulary_size).  In this model, the vocab size is 51200.\n",
        "Each token in the vocabulary is assigned a value that is roughly the probability of that value being next in the sequence.\n",
        "We can see for our sequence what the top ten next tokens were by finding the tokens with the highest values.\n"
      ],
      "metadata": {
        "id": "J7Ca4c6NXk3u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "single_forward_pass = model.forward(**inputs) # perform one forward pass\n",
        "\n",
        "print(f\"Shape of outputs: {single_forward_pass.logits.shape}\\n\\n\")\n",
        "\n",
        "top_10_token_ids = single_forward_pass.logits[0, -1, :].cpu().argsort().tolist()[-10:][::-1] # get the top 10 tokens in the vocabulary from the output tensor\n",
        "top_10_tokens = [tokenizer.decode(token) for token in top_10_token_ids] # decode them back to strings\n",
        "top_10_probs = single_forward_pass.logits[0, -1, :].cpu()[top_10_token_ids] # get their values from the output of the model\n",
        "\n",
        "print(\"Top 10 next possible tokens given our input:\\n\")\n",
        "print(\"token        ~probability\")\n",
        "print(\"-\"*25)\n",
        "for token, prob in zip(top_10_tokens, top_10_probs):\n",
        "    print(f\"{repr(token)[1:-1]: <7} ----> {prob: >11}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "id": "2SbW1CCX76AQ",
        "outputId": "8c31c0a6-b5f7-4fb8-e33c-ddd7ec0b4c79"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of outputs: torch.Size([1, 12, 51200])\n",
            "\n",
            "\n",
            "Top 10 next possible tokens given our input:\n",
            "\n",
            "token        ~probability\n",
            "-------------------------\n",
            ",       ---->    16.65625\n",
            " quickly ---->     16.4375\n",
            " knew   ---->   16.296875\n",
            " and    ---->     16.1875\n",
            " couldn ---->     16.1875\n",
            " swiftly ---->    16.03125\n",
            " skill  ---->  15.9140625\n",
            " decided ---->   15.828125\n",
            " took   ---->  15.7578125\n",
            " of     ---->   15.640625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Helper functions for getting and displaying attention weights\n",
        "\n",
        "def get_attn_weights(inputs, layer, head):\n",
        "    x = model.layers[0](**inputs)\n",
        "    for i in range(1, layer):\n",
        "        x = model.layers[i](x)\n",
        "    x = model.layers[layer].ln(x)\n",
        "    model.layers[layer].mixer\n",
        "    qkv = model.layers[layer].mixer.Wqkv(x)\n",
        "    qkv = rearrange(qkv, \"... (three h d) -> ... three h d\", three=3, d=model.layers[layer].mixer.head_dim)\n",
        "    qkv = model.layers[layer].mixer.rotary_emb(qkv)\n",
        "    batch_size, seqlen = qkv.shape[0], qkv.shape[1]\n",
        "    q, k, v = qkv.unbind(dim=2)\n",
        "    softmax_scale = 1.0 / math.sqrt(q.shape[-1])\n",
        "    scores = torch.einsum('bthd,bshd->bhts', q, k * softmax_scale)\n",
        "    causal_mask = torch.triu(torch.full(size=(seqlen, seqlen), fill_value=-10000.0, device=scores.device), 1)\n",
        "    scores = scores + causal_mask.to(dtype=scores.dtype)\n",
        "    attention = torch.softmax(scores, dim=-1, dtype=v.dtype)\n",
        "    output = torch.einsum('bhts,bshd->bthd', attention, v)\n",
        "    weights = attention[0, head].cpu()\n",
        "    return weights\n",
        "\n",
        "\n",
        "def display_attention_weights(inputs, layer, head, token_idx):\n",
        "    input_tokens = [tokenizer.decode(id) for id in inputs[\"input_ids\"][0]]\n",
        "    weights = get_attn_weights(inputs, layer, head)\n",
        "    with out:\n",
        "        fig, ax = plt.subplots(figsize=(3, 1*(len(input_tokens)//4)))\n",
        "        ax.axis('off')\n",
        "        tl = len(input_tokens)\n",
        "        ax.set_ylim(0, len(input_tokens))\n",
        "        ax.set_xlim(0, 10)\n",
        "        for i, token in enumerate(input_tokens):\n",
        "            ax.text(3, len(input_tokens)-i, token, ha='right', va='top')\n",
        "            ax.text(8, len(input_tokens)-i, token, ha='left', va='top')\n",
        "        ax.fill_between([0, 3.3], [tl-token_idx, tl-token_idx], [tl-token_idx-0.75, tl-token_idx-0.75], color='blue', alpha=0.4)\n",
        "        for i, weight in enumerate(weights[token_idx].cpu().tolist()):\n",
        "            ax.fill_between([7.7, 13], [tl-i, tl-i], [tl-i-0.75, tl-i-0.75], color='blue', alpha=math.sqrt(weight)*0.7)\n",
        "            ax.plot([3.35, 7.65], [tl-token_idx - 0.375, tl-i], c=\"blue\", alpha=math.sqrt(weight)*0.7, lw=0.5)\n",
        "        out.clear_output()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "def handler(_):\n",
        "    display_attention_weights(inputs, layer.value, head.value, token_idx.value)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "U3N5P2plqaOX",
        "outputId": "5be6c18d-cd18-4cba-f458-5999e41fb48e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Select the Layer, Attention Head, and Token to view the attention weights\n",
        "layer = widgets.Dropdown(options=list(range(1, 24)), description=\"Layer\")\n",
        "head = widgets.Dropdown(options=list(range(0, 32)), description=\"Attn Head:\")\n",
        "token_idx = widgets.Dropdown(options=list(zip([tokenizer.decode(id) for id in inputs[\"input_ids\"][0]], list(range(len(inputs[\"input_ids\"][0]))))), description=\"Token:\")\n",
        "button = widgets.Button(description=\"Plot\")\n",
        "button.on_click(handler)\n",
        "\n",
        "out = widgets.Output()\n",
        "\n",
        "display(layer, head, token_idx, button)\n",
        "display(out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141,
          "referenced_widgets": [
            "ef53cafd28984bfa92c6ec0d7cb05ffd",
            "8b7f72ce9aa34607bac860038dfe8d9e",
            "059ad292a6fa469b8b16cfce6684a1ac",
            "3e5437e3abca48a1888c68d27060c080",
            "15dc91ae7dec4da098324d639b5e6214",
            "3196b4ca48b247e090e2a8320f85e700",
            "0d6490833634451b931856e846bf213a",
            "901cecf6802d4e2785bbd8aaa2717480",
            "cba42c425b44421990f1cc5a99b3cf9d",
            "5a7c6a44e7ea43fd9ab61b6d8e4d744f",
            "d1bba53a1d6a4a6282296b970809f39f",
            "dc3b83b76fae421f9f4fac89e34d48c0",
            "19fc5a57d04142dba9a53cf794c4317e",
            "57c8dc9026e84580a3f041cfed0f8b3c"
          ]
        },
        "id": "X-w1l_PERmDv",
        "outputId": "338dc5cc-6ea6-466d-eb1b-b967732d4410"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Dropdown(description='Layer', options=(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, …"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ef53cafd28984bfa92c6ec0d7cb05ffd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Dropdown(description='Attn Head:', options=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, …"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3e5437e3abca48a1888c68d27060c080"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Dropdown(description='Token:', options=(('as', 0), (' it', 1), (' started', 2), (' to', 3), (' sway', 4), (','…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0d6490833634451b931856e846bf213a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Button(description='Plot', style=ButtonStyle())"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5a7c6a44e7ea43fd9ab61b6d8e4d744f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "19fc5a57d04142dba9a53cf794c4317e"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Viewing the embedding dimension values across layers\n",
        "\n",
        "In the following visualization, you can see the embedding dimension being modified and adjusted over each layer.  Here we are only showing the first 5 heads worth of embedding dimension (64*5 = 320).  The red dashed lines represent separations in the values that each head attends to."
      ],
      "metadata": {
        "id": "75DscqqTROf9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Code to generate the following visualization\n",
        "# get the number of total heads and the head dimension\n",
        "_ = \"\"\"\n",
        "n_head = model.layers[1].mixer.n_head\n",
        "head_dim = model.layers[1].mixer.head_dim\n",
        "\n",
        "# get a color palette and shuffle it\n",
        "pal = sns.cubehelix_palette(5, rot=-.6, gamma=0.7, hue=0.7)\n",
        "random.shuffle(pal)\n",
        "\n",
        "# initialize the plot\n",
        "fig, ax = plt.subplots()\n",
        "_ = ax.set_ylim([-2, 2])\n",
        "_ = ax.set_xlabel(\"Embedding dimension position\")\n",
        "_ = ax.set_ylabel(\"Embedding value\")\n",
        "sns.despine()\n",
        "\n",
        "# calculate the x_values, y_values and colors for each bar initlaly\n",
        "x_vals = range(0, 5*head_dim)\n",
        "y_vals = [x[i*head_dim:i*head_dim+head_dim] for i in range(5)]\n",
        "heights = [i for head in y_vals for i in head]\n",
        "colors = [color for head in [[pal[i]]*head_dim for i in range(5)] for color in head]\n",
        "\n",
        "# add the bars, text, and dashed lines\n",
        "bars = plt.bar(x_vals, heights, color=colors)\n",
        "text_label = ax.text(0.7, 0.9, '', transform=ax.transAxes)\n",
        "for x_val in range(head_dim, 5*64, 64):\n",
        "    _ = ax.axvline(x=x_val, color='r', linestyle='--', lw=0.5)\n",
        "\n",
        "\n",
        "prev_heights = None\n",
        "interpolation_steps = 10\n",
        "\n",
        "def update(frame):\n",
        "    global prev_heights\n",
        "\n",
        "    real_frame = frame // interpolation_steps\n",
        "    interp = frame % interpolation_steps / interpolation_steps\n",
        "\n",
        "    x = model.layers[0](**inputs)\n",
        "    for i in range(1, real_frame):\n",
        "        x = model.layers[i](x)\n",
        "    x = model.layers[layer].ln(x)\n",
        "    x = model.layers[layer].mixer(x)\n",
        "    x = x[0, -1].tolist()\n",
        "    y_vals = [x[i * head_dim:i * head_dim + head_dim] for i in range(5)]\n",
        "    heights = [i for head in y_vals for i in head]\n",
        "\n",
        "    if prev_heights is not None:\n",
        "        # Perform the linear interpolation between the previous and current frame.\n",
        "        heights = [(1 - interp) * prev + interp * curr for prev, curr in zip(prev_heights, heights)]\n",
        "\n",
        "\n",
        "    for i, bar in enumerate(bars):\n",
        "        bar.set_height(heights[i])\n",
        "\n",
        "    # Update the text label\n",
        "    text_label.set_text(f'Model Layer: {real_frame + 1}')\n",
        "\n",
        "    # Store the current heights for the next frame\n",
        "    prev_heights = heights\n",
        "\n",
        "    return bars\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "# uncomment the following lines to run this cell\n",
        "# ani = animation.FuncAnimation(fig, update, frames=24 * interpolation_steps, blit=True, interval=400//interpolation_steps)\n",
        "# HTML(ani.to_html5_video())\n",
        "# ani.save('animation.mp4', writer='ffmpeg', fps=30)\n",
        "# from google.colab import files\n",
        "# files.download('animation.mp4')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "cellView": "form",
        "id": "9akgzRXM1Ubs",
        "outputId": "391b6047-a0bb-4bbc-eb44-18567b0833f8"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "HTML(f\"\"\"<video src=https://github.com/tim-a-davis/silly_little_language_modeling_thing_at_utd/raw/main/animation.mp4 width=700 controls loop/>\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 546
        },
        "id": "I1_SWYoh1Tuj",
        "outputId": "e331a5af-e61f-4513-d217-ed662384816b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<video src=https://github.com/tim-a-davis/silly_little_language_modeling_thing_at_utd/raw/main/animation.mp4 width=700 controls loop/>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tear Down"
      ],
      "metadata": {
        "id": "gmJKUNhEf5wI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "del model\n",
        "del outputs\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "6R1yxBAk3krS",
        "outputId": "93479bc8-6ec3-43ce-a851-ceebe7986114"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chat-tuned Models"
      ],
      "metadata": {
        "id": "Pdo-Yni8ZpT-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\"teknium/Puffin-Phi-v2\", trust_remote_code=True, torch_dtype=torch.bfloat16)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"teknium/Puffin-Phi-v2\", trust_remote_code=True, torch_dtype=torch.bfloat16)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "yhwi25jkjlC_",
        "outputId": "5a846324-710a-4b5b-af83-bb575ae26f54"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sysprompt = \"Be an old-timey sailor on a ship in the style of herman melville.\\n\"\n",
        "inputs = tokenizer(f\"{sysprompt}USER: What is a master-at-arms?  Give me a short answer.\\nASSISTANT:\", return_tensors=\"pt\", return_attention_mask=False)\n",
        "outputs = model.generate(**inputs, max_new_tokens=120, temperature=0.99, do_sample=True, use_cache=True, repetition_penalty=1.2, eos_token_id=tokenizer.eos_token_id)\n",
        "text = tokenizer.batch_decode(outputs)[0]\n",
        "\n",
        "print(\"\\n\\nOutput:\\n\\n\", text)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "id": "Ny0_AxO6Zvsl",
        "outputId": "f4c3ee71-978d-4b7e-999d-7afda402ea59"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Output:\n",
            "\n",
            " Be an old-timey sailor on a ship in the style of herman melville.\n",
            "USER: What is a master-at-arms?  Give me a short answer.\n",
            "ASSISTANT: A master-at-arms, also known as a reggae soldier or simply a \"rebeccap,\" was a pivotal figure during the era of US colonial America (1775-1822). They typically played several roles and were highly skilled warriors with knowledge of various combat styles. Often serving under other well-known figures such as John Brown or Paul Revere, they facilitated communication between the American colonies and their allies during times of conflict<|endoftext|>\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "collapsed_sections": [
        "_pwlS5rkTAIh"
      ],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ef53cafd28984bfa92c6ec0d7cb05ffd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DropdownModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DropdownModel",
            "_options_labels": [
              "1",
              "2",
              "3",
              "4",
              "5",
              "6",
              "7",
              "8",
              "9",
              "10",
              "11",
              "12",
              "13",
              "14",
              "15",
              "16",
              "17",
              "18",
              "19",
              "20",
              "21",
              "22",
              "23"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "DropdownView",
            "description": "Layer",
            "description_tooltip": null,
            "disabled": false,
            "index": 0,
            "layout": "IPY_MODEL_8b7f72ce9aa34607bac860038dfe8d9e",
            "style": "IPY_MODEL_059ad292a6fa469b8b16cfce6684a1ac"
          }
        },
        "8b7f72ce9aa34607bac860038dfe8d9e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "059ad292a6fa469b8b16cfce6684a1ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3e5437e3abca48a1888c68d27060c080": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DropdownModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DropdownModel",
            "_options_labels": [
              "0",
              "1",
              "2",
              "3",
              "4",
              "5",
              "6",
              "7",
              "8",
              "9",
              "10",
              "11",
              "12",
              "13",
              "14",
              "15",
              "16",
              "17",
              "18",
              "19",
              "20",
              "21",
              "22",
              "23",
              "24",
              "25",
              "26",
              "27",
              "28",
              "29",
              "30",
              "31"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "DropdownView",
            "description": "Attn Head:",
            "description_tooltip": null,
            "disabled": false,
            "index": 0,
            "layout": "IPY_MODEL_15dc91ae7dec4da098324d639b5e6214",
            "style": "IPY_MODEL_3196b4ca48b247e090e2a8320f85e700"
          }
        },
        "15dc91ae7dec4da098324d639b5e6214": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3196b4ca48b247e090e2a8320f85e700": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0d6490833634451b931856e846bf213a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DropdownModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DropdownModel",
            "_options_labels": [
              "as",
              " it",
              " started",
              " to",
              " sway",
              ",",
              " the",
              " master",
              "-",
              "at",
              "-",
              "arms"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "DropdownView",
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "index": 0,
            "layout": "IPY_MODEL_901cecf6802d4e2785bbd8aaa2717480",
            "style": "IPY_MODEL_cba42c425b44421990f1cc5a99b3cf9d"
          }
        },
        "901cecf6802d4e2785bbd8aaa2717480": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cba42c425b44421990f1cc5a99b3cf9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5a7c6a44e7ea43fd9ab61b6d8e4d744f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Plot",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_d1bba53a1d6a4a6282296b970809f39f",
            "style": "IPY_MODEL_dc3b83b76fae421f9f4fac89e34d48c0",
            "tooltip": ""
          }
        },
        "d1bba53a1d6a4a6282296b970809f39f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc3b83b76fae421f9f4fac89e34d48c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "19fc5a57d04142dba9a53cf794c4317e": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_57c8dc9026e84580a3f041cfed0f8b3c",
            "msg_id": "",
            "outputs": []
          }
        },
        "57c8dc9026e84580a3f041cfed0f8b3c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}