{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tim-a-davis/silly_little_language_modeling_thing_at_utd/blob/main/CurtGPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What is a language model"
      ],
      "metadata": {
        "id": "yAShZEifWVAV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*italicized text*# New Section"
      ],
      "metadata": {
        "id": "2ux4R7erWT4B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from collections import defaultdict, Counter\n",
        "import random\n",
        "import time"
      ],
      "metadata": {
        "id": "miP5UKcBae7u"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML, display\n",
        "\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ],
      "metadata": {
        "id": "vpc7B03YLyP1"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TrigramModel:\n",
        "    def __init__(self, url):\n",
        "        self.trigram_freq = defaultdict(Counter)\n",
        "        self._train(url)\n",
        "\n",
        "    def _train(self, url):\n",
        "        r = requests.get(url)\n",
        "        text = r.text.lower().split()\n",
        "\n",
        "        # Create trigrams\n",
        "        for i in range(len(text) - 2):\n",
        "            trigram = (text[i], text[i + 1], text[i + 2])\n",
        "            self.trigram_freq[(trigram[0], trigram[1])][trigram[2]] += 1\n",
        "\n",
        "    def _get_weighted_random_word(self, counter):\n",
        "        total = sum(counter.values())\n",
        "        random_choice = random.randint(1, total)\n",
        "\n",
        "        for word, freq in counter.items():\n",
        "            random_choice -= freq\n",
        "            if random_choice <= 0:\n",
        "                return word\n",
        "\n",
        "    def predict(self, text, n_words):\n",
        "        words = text.lower().split()\n",
        "        output = words.copy()\n",
        "\n",
        "        for _ in range(n_words):\n",
        "            last_bigram = tuple(output[-2:])\n",
        "            if last_bigram in self.trigram_freq:\n",
        "                next_word = self._get_weighted_random_word(\n",
        "                    self.trigram_freq[last_bigram]\n",
        "                )\n",
        "                output.append(next_word)\n",
        "            else:\n",
        "                break\n",
        "\n",
        "        return \" \".join(output)\n",
        "\n",
        "    def get_frequencies_of_bigram(self, text):\n",
        "        words = text.lower().split()\n",
        "        bigram = tuple(words[-2:])\n",
        "        return bigram, self.trigram_freq[bigram]\n"
      ],
      "metadata": {
        "id": "PTRO7WnoagbC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "ffe95970-07ba-4089-833e-dfbe1cca08cc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = TrigramModel(\"http://gutenberg.net.au/ebooks06/0608511.txt\")"
      ],
      "metadata": {
        "id": "7-eYXbtwg8_m",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "f6b1c646-64bc-4278-8168-460e453d1682"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"as it started to sway, the master-at-arms\"\n",
        "n_words = 50  # Number of words ahead to predict\n",
        "\n",
        "prediction = model.predict(text, n_words)\n",
        "for i, letter in enumerate(prediction):\n",
        "    if not i % 100: print(\"\\n\")\n",
        "    print(letter, end='', flush=True)\n",
        "    time.sleep(0.003)"
      ],
      "metadata": {
        "id": "AW--CcH2hILO",
        "outputId": "4a08b532-d4c8-49ce-8f83-186051e4c29a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "as it started to sway, the master-at-arms of a singing-bird on the victor of waterloo ventures not t\n",
            "\n",
            "o handle such breadths of heavy canvas as the handsome sailor, merrily joined in; then addressing hi\n",
            "\n",
            "s messmates exclaimed, \"there now, who says that jimmy legs is down on you.\" \"and what,\" rejoined bi\n",
            "\n",
            "lly in spilling the soup just when"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model.get_frequencies_of_bigram(text)"
      ],
      "metadata": {
        "id": "MGT9pkrthKSO",
        "outputId": "dc8ceffb-384f-4871-dd29-ab2e3cb6e4ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(('the', 'master-at-arms'),\n",
              " Counter({'of': 1,\n",
              "          'was': 4,\n",
              "          'has': 1,\n",
              "          'in': 1,\n",
              "          'noticed': 1,\n",
              "          'that': 1,\n",
              "          'never': 1,\n",
              "          'being': 1,\n",
              "          'acted': 1,\n",
              "          'about': 1,\n",
              "          'said.': 1,\n",
              "          'said': 1,\n",
              "          'as': 1,\n",
              "          'and': 1}))"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![overly-complicated-diagram](http://www.phon.ox.ac.uk/jcoleman/old_SLP/Lecture_6/figure7-8.png)\n",
        "\n",
        "http://www.phon.ox.ac.uk/jcoleman/old_SLP/Lecture_6/trigram-modelling.html"
      ],
      "metadata": {
        "id": "3M4k-xVvk-b3"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BEs97SPLlFSt"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C3rIdMD4I2XW"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "64t_XH5iI2ZW"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sG22I_qQI2br"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1OXBNjsfI2eA"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hS2bqqkQI2gg"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q trl transformers accelerate peft datasets bitsandbytes einops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "AdJ2VEOjI2if",
        "outputId": "b2933288-76ac-462f-85b0-8f0b85fc8281"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.0/118.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.1/258.1 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.6/85.6 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.6/519.6 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.8/294.8 kB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "o8ptIpRUNVul",
        "outputId": "bfed3495-40d0-4836-ac23-c0f2ddfe3ba8"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.set_default_device('cuda')\n",
        "model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-1_5\", trust_remote_code=True, torch_dtype=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-1_5\", trust_remote_code=True, torch_dtype=\"auto\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "IDMwICNuN3mS",
        "outputId": "861dcfbb-3164-4a53-fa5c-838184d379b9"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(\"For both pretraining and finetuning, we concat\", return_tensors=\"pt\", return_attention_mask=False)\n",
        "print(inputs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "id": "5kjTm5iSWXSc",
        "outputId": "7ebc8e92-3aad-41b5-eb9a-f6b1659b2743"
      },
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[ 1890,  1111,  2181, 24674,   290,   957,   316, 46493,    11,   356,\n",
            "          1673,   265]], device='cuda:0')}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for token_id in inputs[\"input_ids\"][0]:\n",
        "    id = token_id.item()\n",
        "    token = tokenizer.decode(id)\n",
        "    print(f\"{id: <5} ----> {token}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "pjE2ZcWPtC3C",
        "outputId": "290131aa-ca6d-40c1-9d33-d8561745bfd8"
      },
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1890  ----> For\n",
            "1111  ---->  both\n",
            "2181  ---->  pret\n",
            "24674 ----> raining\n",
            "290   ---->  and\n",
            "957   ---->  fin\n",
            "316   ----> et\n",
            "46493 ----> uning\n",
            "11    ----> ,\n",
            "356   ---->  we\n",
            "1673  ---->  conc\n",
            "265   ----> at\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(**inputs, max_new_tokens=20)\n",
        "output_tokens = [tokenizer.decode(id) for id in outputs[0]]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "LCNfBF_0WXUX",
        "outputId": "c696aa79-7a91-4021-9128-71a15c821936"
      },
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def print_tokens(ids, tokens):\n",
        "    tokens = [token.replace(\" \", \"·\") for token in tokens]\n",
        "    max_widths = [max(len(str(id)), len(token)) for id, token in zip(ids, tokens)]\n",
        "    aligned_ids = [str(id).center(max_widths[i]) for i, id in enumerate(ids)]\n",
        "    aligned_arrows = ['↓'.center(max_widths[i]) for i in range(len(ids))]\n",
        "    aligned_tokens = [token.center(max_widths[i]) for i, token in enumerate(tokens)]\n",
        "    print(' '.join(aligned_ids))\n",
        "    print(' '.join(aligned_arrows))\n",
        "    print(repr(' '.join(aligned_tokens))[1:-1])\n",
        "\n",
        "\n",
        "print(\"Output:\\n\" + \"\".join(output_tokens) + \"\\n\\nToken Mapping:\")\n",
        "print_tokens(outputs.cpu().tolist()[0], output_tokens)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "id": "4JHmujrpqUh-",
        "outputId": "04d37cee-5ea1-456f-c63b-3997c52f4b51"
      },
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "For both pretraining and finetuning, we concat the input and output of the encoder and decoder, respectively.\n",
            "\n",
            "```python\n",
            "\n",
            "\n",
            "Token Mapping:\n",
            "1890  1111  2181  24674  290  957  316 46493 11 356  1673 265 262   5128  290    5072  286 262  2207 12342 290  875  12342 11      8148     13 198 198 15506 63 29412  198\n",
            " ↓     ↓     ↓      ↓     ↓    ↓    ↓    ↓   ↓   ↓    ↓    ↓   ↓     ↓     ↓      ↓     ↓   ↓    ↓     ↓    ↓    ↓     ↓   ↓        ↓       ↓   ↓   ↓    ↓   ↓    ↓     ↓ \n",
            "For  ·both ·pret raining ·and ·fin  et uning ,  ·we ·conc  at ·the ·input ·and ·output ·of ·the ·enc  oder ·and ·dec  oder ,  ·respectively .   \\n   \\n    ``  `  python  \\n \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize input tensor `x` (size: [batch_size, sequence_length])\n",
        "x = ...\n",
        "\n",
        "# Forward through Embedding layer\n",
        "x = model.layers[0](x)\n",
        "\n",
        "# Forward through ParallelBlock layers\n",
        "for i in range(1, 25):  # Assuming 24 ParallelBlock layers are there\n",
        "    x = model.layers[i](x)\n",
        "\n",
        "# Now `x` contains the tensor before it goes into the `CausalLMHead` layer.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "U3N5P2plqaOX",
        "outputId": "107bf7a6-7bb2-44e2-b270-aecfbcfcbf6e"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.forward(**inputs).logits.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "T464Amnj3QO4",
        "outputId": "fb7b6cc5-0d71-4278-e682-1ebeccfdcbaf"
      },
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 12, 51200])"
            ]
          },
          "metadata": {},
          "execution_count": 176
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "6R1yxBAk3krS",
        "outputId": "8ea0ee5f-a386-40e1-ae1d-a8e4a75af75b"
      },
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 12])"
            ]
          },
          "metadata": {},
          "execution_count": 178
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}