{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tim-a-davis/silly_little_language_modeling_thing_at_utd/blob/main/CurtGPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<table>\n",
        "<tr>\n",
        "<td style=\"width: 10%;\">\n",
        "\n",
        "# CurtGPT\n",
        "Using Microsoft's Phi 1.5 model like it was never intended.\n",
        "\n",
        "</td>\n",
        "<td style=\"text-align: center;\">\n",
        "<img src=\"https://github.com/tim-a-davis/silly_little_language_modeling_thing_at_utd/blob/main/curtgpt%20logo.png?raw=true\" width=\"300\" height=\"auto\">\n",
        "</td>\n",
        "</tr>\n",
        "</table>\n"
      ],
      "metadata": {
        "id": "AgN0tXTQ5SVS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup, Installs, Imports\n",
        "\n",
        "Setup of the environment, installation of the needed models, and importing everything required to run the notebook"
      ],
      "metadata": {
        "id": "G6hseI_DS0H2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Installing dependencies\n",
        "!pip install -q trl transformers accelerate peft datasets bitsandbytes einops"
      ],
      "metadata": {
        "id": "eJQmTHR3dIrL"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Imports & setup\n",
        "from IPython.display import HTML, display, clear_output\n",
        "import ipywidgets as widgets\n",
        "\n",
        "import requests\n",
        "import random\n",
        "import time\n",
        "import math\n",
        "import numba\n",
        "import numpy as np\n",
        "\n",
        "from collections import defaultdict, Counter\n",
        "from typing import List, Callable\n",
        "from einops import rearrange\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
        "\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "import seaborn as sns\n",
        "\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ],
      "metadata": {
        "id": "miP5UKcBae7u",
        "cellView": "form"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install Phi models and tokenizer\n",
        "AutoModelForCausalLM.from_pretrained(\"microsoft/phi-1_5\", trust_remote_code=True, torch_dtype=\"auto\")\n",
        "AutoTokenizer.from_pretrained(\"microsoft/phi-1_5\", trust_remote_code=True, torch_dtype=\"auto\")\n",
        "\n",
        "AutoModelForCausalLM.from_pretrained(\"teknium/Puffin-Phi-v2\", trust_remote_code=True, torch_dtype=torch.bfloat16)\n",
        "AutoTokenizer.from_pretrained(\"teknium/Puffin-Phi-v2\", trust_remote_code=True, torch_dtype=torch.bfloat16)\n",
        "\n",
        "torch.set_default_device('cuda')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38
        },
        "id": "HzJsUkpeTab4",
        "outputId": "ac880385-079d-4e4f-cfcd-2ca930002cd8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# N-Gram Models\n",
        "\n",
        "Starting with n-gram models will hopfully build some intuition on language modeling in general."
      ],
      "metadata": {
        "id": "_pwlS5rkTAIh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building the Trigram Model\n",
        "---\n",
        "Here we write all the code we'll need to ingest a corpus of text and create a representation of that text in the form of a trigram model.  The main idea behind the trigram model is fundementally the same as with decoder-only tranformer based models, but trigram models are easier to disect, so we'll start there."
      ],
      "metadata": {
        "id": "lX2tBK0fUZRC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TrigramModel:\n",
        "    def __init__(self, url):\n",
        "        self.trigram_freq = defaultdict(Counter)\n",
        "        self._train(url)\n",
        "\n",
        "    def _train(self, url):\n",
        "        r = requests.get(url)\n",
        "        text = r.text.lower().split()\n",
        "\n",
        "        # Create trigrams\n",
        "        for i in range(len(text) - 2):\n",
        "            trigram = (text[i], text[i + 1], text[i + 2])\n",
        "            self.trigram_freq[(trigram[0], trigram[1])][trigram[2]] += 1\n",
        "\n",
        "    def _get_weighted_random_word(self, counter):\n",
        "        total = sum(counter.values())\n",
        "        random_choice = random.randint(1, total)\n",
        "\n",
        "        for word, freq in counter.items():\n",
        "            random_choice -= freq\n",
        "            if random_choice <= 0:\n",
        "                return word\n",
        "\n",
        "    def predict(self, text, n_words):\n",
        "        words = text.lower().split()\n",
        "        output = words.copy()\n",
        "\n",
        "        for _ in range(n_words):\n",
        "            last_bigram = tuple(output[-2:])\n",
        "            if last_bigram in self.trigram_freq:\n",
        "                next_word = self._get_weighted_random_word(\n",
        "                    self.trigram_freq[last_bigram]\n",
        "                )\n",
        "                output.append(next_word)\n",
        "            else:\n",
        "                break\n",
        "\n",
        "        return \" \".join(output)\n",
        "\n",
        "    def get_frequencies_of_bigram(self, text):\n",
        "        words = text.lower().split()\n",
        "        bigram = tuple(words[-2:])\n",
        "        return bigram, self.trigram_freq[bigram]\n"
      ],
      "metadata": {
        "id": "PTRO7WnoagbC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38
        },
        "outputId": "b06431d4-c3d1-4ec1-b9ca-c8520e5d99f4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Instantiate The Model\n",
        "---\n",
        "\n",
        "Let's instantiate this trigram model with a .txt file containing the book _Billy Budd, Sailor_ by _Herman Melville_"
      ],
      "metadata": {
        "id": "y7Ok1o9GU0fj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = TrigramModel(\"http://gutenberg.net.au/ebooks06/0608511.txt\")"
      ],
      "metadata": {
        "id": "7-eYXbtwg8_m",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38
        },
        "outputId": "c6ed1140-e472-40e2-acfa-7f9ce64bcacd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "Here we prompt this model with some starting text, and we want to see what the model things the next n_words will be.  Given the way we've tokenized the text (splitting on spaces), the trigram model will only be looking at the last 2 words (in this case `the, master-at-arms`).  Then we add some small delay to make it look like a sweet streaming GPT model"
      ],
      "metadata": {
        "id": "yFYADHGCVHuq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"as it started to sway, the master-at-arms\"\n",
        "n_words = 50  # Number of words ahead to predict\n",
        "\n",
        "prediction = model.predict(prompt, n_words)\n",
        "for i, letter in enumerate(prediction):\n",
        "    if not i % 100: print(\"\\n\")\n",
        "    print(letter, end='', flush=True)\n",
        "    time.sleep(0.003)"
      ],
      "metadata": {
        "id": "AW--CcH2hILO",
        "outputId": "e0f487e9-7cb1-4bed-f78c-a7e621bf698a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "as it started to sway, the master-at-arms and the old-fashioned sailor, the commander of the heart n\n",
            "\n",
            "ot the less to do quite as much as he was everything that a young man if of the honest sense of fear\n",
            "\n",
            ", his apprehension as to the gazer's professional eye it was the most important regards ceased to be\n",
            "\n",
            " called,"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Disecting one of the bigrams\n",
        "\n",
        "---\n",
        "\n",
        "Here we can take a look at the models choice of words for our example bigram.  The output shows the bigram, as well as the frequencies of words found in the text.  "
      ],
      "metadata": {
        "id": "oPwJRKi6Vv1T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.get_frequencies_of_bigram(prompt)"
      ],
      "metadata": {
        "id": "MGT9pkrthKSO",
        "outputId": "28e656e3-b795-4d32-b385-48843744366d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 798
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(('the', 'master-at-arms'),\n",
              " Counter({'of': 1,\n",
              "          'was': 4,\n",
              "          'has': 1,\n",
              "          'in': 1,\n",
              "          'noticed': 1,\n",
              "          'that': 1,\n",
              "          'never': 1,\n",
              "          'being': 1,\n",
              "          'acted': 1,\n",
              "          'about': 1,\n",
              "          'said.': 1,\n",
              "          'said': 1,\n",
              "          'as': 1,\n",
              "          'and': 1}))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Confusing diagram"
      ],
      "metadata": {
        "id": "EYKl9KV7WC-s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![overly-complicated-diagram](http://www.phon.ox.ac.uk/jcoleman/old_SLP/Lecture_6/figure7-8.png)\n",
        "\n",
        "http://www.phon.ox.ac.uk/jcoleman/old_SLP/Lecture_6/trigram-modelling.html"
      ],
      "metadata": {
        "id": "3M4k-xVvk-b3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Base Transformer Model -- Phi-1.5\n",
        "\n",
        "---\n",
        "\n",
        "Here we'll start to look at a base transformer model to understand a little bit about how it behaves, how it's similar to a trigram model, and how we can use some tools to easily interact with these extremely large models."
      ],
      "metadata": {
        "id": "gRPuQCI_UM7F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Downloading and Instantiating\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "This part is not difficult thanks to the great work at huggingface.  With just two lines of code we can download a 1.3 billion parameter transformer model, map the weights onto the architecture, and load the associated configurations and tokenizers."
      ],
      "metadata": {
        "id": "mbRGL_SJWZzg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-1_5\", trust_remote_code=True, torch_dtype=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-1_5\", trust_remote_code=True, torch_dtype=\"auto\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38
        },
        "id": "IDMwICNuN3mS",
        "outputId": "10467ed4-5351-4caf-91f3-9077637963ac"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompting Phi 1.5\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Here we take our same prompt, and we use the model's tokenizer to turn it into integers that the model can ingest.\n",
        "The result will be a string of integers that represent chunks of text we call tokens."
      ],
      "metadata": {
        "id": "HJ2ta6FjW2FI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"as it started to sway, the master-at-arms\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\", return_attention_mask=False)\n",
        "print(inputs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        },
        "id": "5kjTm5iSWXSc",
        "outputId": "c0a1d518-8815-4c6d-8b93-cd841047a132"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[  292,   340,  2067,   284, 20009,    11,   262,  4958,    12,   265,\n",
            "            12,  8357]], device='cuda:0')}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "We can use the tokenizer to reverse the process and get back the strings.\n",
        "You can see that the tokenizer sometimes chooses interesting places to chunk the text.\n",
        "You can also see that generally speaking, more common tokens have lower integer values.\n",
        "\n",
        "Here we show the integer values of each token and what the string representation is for each input."
      ],
      "metadata": {
        "id": "YtgrA9f0W92D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for token_id in inputs[\"input_ids\"][0]:\n",
        "    id = token_id.item()\n",
        "    token = tokenizer.decode(id)\n",
        "    print(f\"{id: <5} ----> {token}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pjE2ZcWPtC3C",
        "outputId": "6e1ea623-7759-4080-cef6-6bf4aa0df847"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "292   ----> as\n",
            "340   ---->  it\n",
            "2067  ---->  started\n",
            "284   ---->  to\n",
            "20009 ---->  sway\n",
            "11    ----> ,\n",
            "262   ---->  the\n",
            "4958  ---->  master\n",
            "12    ----> -\n",
            "265   ----> at\n",
            "12    ----> -\n",
            "8357  ----> arms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Performing inference on this model is as simple as passing in the inputs from the tokenizer to the .generate method.\n",
        "\n",
        "The tokenizer has a batch_decode method that we would generally use to get back an output text.  But in this case\n",
        "we want to see each individual token and what the model output was.\n",
        "\n"
      ],
      "metadata": {
        "id": "WrZRnxhQXDp4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(**inputs, max_new_tokens=11)\n",
        "output_tokens = [tokenizer.decode(id) for id in outputs[0]]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38
        },
        "id": "LCNfBF_0WXUX",
        "outputId": "d5cabd32-c7be-4e2e-d615-f87cd9465a8a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Helper function for printing token ids and tokens\n",
        "def print_tokens(ids, tokens, line_size=25):\n",
        "    tokens = [token.replace(\" \", \"·\") for token in tokens]\n",
        "    def chunk_list(lst, max_size):\n",
        "        for i in range(0, len(lst), max_size):\n",
        "            yield lst[i:i + max_size]\n",
        "    id_chunks = list(chunk_list(ids, line_size))\n",
        "    token_chunks = list(chunk_list(tokens, line_size))\n",
        "    for ids, tokens in zip(id_chunks, token_chunks):\n",
        "        max_widths = [max(len(str(id)), len(token)) for id, token in zip(ids, tokens)]\n",
        "        aligned_ids = [str(id).center(max_widths[i]) for i, id in enumerate(ids)]\n",
        "        aligned_arrows = ['↓'.center(max_widths[i]) for i in range(len(ids))]\n",
        "        aligned_tokens = [token.center(max_widths[i]) for i, token in enumerate(tokens)]\n",
        "        print(' '.join(aligned_ids))\n",
        "        print(' '.join(aligned_arrows))\n",
        "        print(repr(' '.join(aligned_tokens))[1:-1])\n",
        "        print(\"\\n\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38
        },
        "cellView": "form",
        "id": "4JHmujrpqUh-",
        "outputId": "e7948eb2-1c65-494a-982e-3a38e015b48b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inspecting Model Outputs\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Here we can decode each token output from the model, and map it back to original text.  We can see that this output is much more congruous with the input prompt over the trigram model.  "
      ],
      "metadata": {
        "id": "POcTMiJGXNs3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"Output:\\n\" + \"\".join(output_tokens) + \"\\n\\nToken Mapping:\")\n",
        "print_tokens(outputs.cpu().tolist()[0], output_tokens)\n",
        "\n",
        "print(\"\\n\\n* The · characters represent spaces in the token\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "iHZ_Zp_wcr-7",
        "outputId": "cd90d214-9113-459d-8299-19c35382389e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "as it started to sway, the master-at-arms, a seasoned veteran of the battlefield, stepped forward.\n",
            "\n",
            "Token Mapping:\n",
            "292 340   2067   284 20009 11 262    4958  12 265 12 8357 11 257   29314     9298   286 262     13480     11  10764     2651   13\n",
            " ↓   ↓     ↓      ↓    ↓   ↓   ↓      ↓    ↓   ↓  ↓   ↓   ↓   ↓      ↓        ↓      ↓   ↓        ↓       ↓     ↓        ↓     ↓ \n",
            " as ·it ·started ·to ·sway ,  ·the ·master -   at -  arms ,   ·a ·seasoned ·veteran ·of ·the ·battlefield ,  ·stepped ·forward . \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "* The · characters represent spaces in the token\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "The model itself outputs a tensor of size (..., sequence_length, vocabulary_size).  In this model, the vocab size is 51200.\n",
        "Each token in the vocabulary is assigned a value that is roughly the probability of that value being next in the sequence.\n",
        "We can see for our sequence what the top ten next tokens were by finding the tokens with the highest values.\n"
      ],
      "metadata": {
        "id": "J7Ca4c6NXk3u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  single_forward_pass = model.forward(**inputs) # perform one forward pass\n",
        "\n",
        "print(f\"Shape of outputs: {single_forward_pass.logits.shape}\\n\\n\")\n",
        "\n",
        "logits = single_forward_pass.logits[0, -1, :].cpu()\n",
        "exp_sum = torch.exp(single_forward_pass.logits[0, -1, :].cpu()).sum().item()\n",
        "top_10_token_ids = single_forward_pass.logits[0, -1, :].cpu().argsort().tolist()[-10:][::-1] # get the top 10 tokens in the vocabulary from the output tensor\n",
        "top_10_tokens = [tokenizer.decode(token) for token in top_10_token_ids] # decode them back to strings\n",
        "top_10_probs = (torch.exp(single_forward_pass.logits[0, -1, :].cpu()[top_10_token_ids])/exp_sum).tolist() # get their probabilities from the output of the model\n",
        "top_10_probs_rounded = [round(i, 5) for i in top_10_probs]\n",
        "\n",
        "\n",
        "print(\"Top 10 next possible tokens given our input:\\n\")\n",
        "print(\"token            probability\")\n",
        "print(\"-\"*28)\n",
        "for token, prob in zip(top_10_tokens, top_10_probs_rounded):\n",
        "    print(f\"{repr(token)[1:-1]: <10} ----> {prob: >11}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2SbW1CCX76AQ",
        "outputId": "31bf6f3d-6579-4e81-90ab-0e85eb2d672b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of outputs: torch.Size([1, 12, 51200])\n",
            "\n",
            "\n",
            "Top 10 next possible tokens given our input:\n",
            "\n",
            "token            probability\n",
            "----------------------------\n",
            ",          ---->     0.04116\n",
            " quickly   ---->     0.03307\n",
            " knew      ---->     0.02873\n",
            " and       ---->     0.02575\n",
            " couldn    ---->     0.02575\n",
            " swiftly   ---->     0.02203\n",
            " skill     ---->     0.01959\n",
            " decided   ---->     0.01798\n",
            " took      ---->     0.01676\n",
            " of        ---->     0.01491\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A great overview of the different sampling algorithms commonly found for LLMs can be found [here](https://towardsdatascience.com/decoding-strategies-that-you-need-to-know-for-response-generation-ba95ee0faadc)"
      ],
      "metadata": {
        "id": "njuAs8nOBhO9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Writing a custom sampler\n",
        "---\n",
        "Decoder-only models produce vocabulary-length array of logits that we can transform into probabilities using a softmax function.  Softmax is defines as:\n",
        "\n",
        "$$\n",
        "\\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j=1}^{n} e^{x_j}}\n",
        "$$\n",
        "\n",
        "Typically, to constrain the range of possible outcomes, samplers use a parameter called temperature to adjust the probability space.  By dividing each logit by T, it makes more probable tokens even _more_ probable, and less probable tokens even _less_ probable. The softmax function with temperature looks like this:\n",
        "\n",
        "$$\n",
        "\\text{softmax}(x_i; T) = \\frac{e^{x_i / T}}{\\sum_{j=1}^{n} e^{x_j / T}}\n",
        "$$\n",
        "\n",
        "\n",
        "You can see then that the base softmax function has a \"default\" temperature of 1.  The following illustration shows how changing the temperature can effect the probabilities:\n",
        "\n",
        "![temp](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i9cXkz-TWG7-BS6CycahuQ.png)"
      ],
      "metadata": {
        "id": "l_dbLJLQZ2Nq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_possible_tokens(prompt, temperature=1, top_p=0.9, top_k=None):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", return_attention_mask=False)\n",
        "    with torch.no_grad():\n",
        "        single_forward_pass = model.forward(**inputs)\n",
        "    logits = single_forward_pass.logits[0, -1, :].cpu() # get the logits from the outputs\n",
        "    logits = logits - torch.max(logits) # subtract the max for numerical stability\n",
        "    exp_sum = torch.exp(logits/temperature).sum().item() # find the sum of the exponentiated logits\n",
        "    probs = torch.exp(logits/temperature)/exp_sum # get the probability from softmax\n",
        "    sort_idx = probs.argsort().tolist()[::-1] # get the index of tokens from most probable to least probable\n",
        "    probs_sorted = probs[sort_idx] # sort the probabilities\n",
        "    top_k_calculated = ((torch.cumsum(probs_sorted, 0) > 0.9) * 1).argmax() + 1 # find the index of the token that crosses the top_p threshold\n",
        "    top_k = top_k or top_k_calculated # override top_k if it is passed as a kwarg\n",
        "    token_ids_considered = sort_idx[:top_k] # get the list of tokens where sum(p) < top_p\n",
        "    token_probs = (probs_sorted[:top_k] / torch.sum(probs_sorted[:top_k])).to(torch.float16)\n",
        "    tokens_considered = tokenizer.batch_decode(token_ids_considered)\n",
        "    return tokens_considered, token_probs\n",
        "\n",
        "\n",
        "def generate_custom(prompt: str,\n",
        "                    custom_sampler: Callable,\n",
        "                    temperature: float=1,\n",
        "                    top_p: float=0.9,\n",
        "                    top_k: float=None,\n",
        "                    max_new_tokens: int=20\n",
        "    ):\n",
        "    for _ in range(max_new_tokens):\n",
        "        tokens_considered, token_probs = get_possible_tokens(prompt, temperature=temperature, top_p=top_p, top_k=top_k)\n",
        "        selected_token = custom_sampler(tokens_considered, token_probs)\n",
        "        prompt += selected_token\n",
        "    return prompt\n",
        "\n",
        "\n",
        "def try_for_certain_letter(tokens: List, token_probs: torch.Tensor, letter: str=\"b\") -> str:\n",
        "    \"\"\"\n",
        "    Given a list of token strings and corresponding probabilities, this function returns a token that starts\n",
        "    with the letter 'p' if any such token exists in the list. The token must also follow a space, meaning it should\n",
        "    represent the beginning of a new word. If no such token is found, a token is selected randomly from the list\n",
        "    based on the given probabilities.\n",
        "\n",
        "    Parameters:\n",
        "    - tokens (list of str): A list of tokens (substrings) to search through.\n",
        "    - token_probs (numpy.ndarray or tensor): An array or tensor of probabilities corresponding to each token in `tokens`.\n",
        "                                              The length of this array should match the length of `tokens`.\n",
        "\n",
        "    Returns:\n",
        "    - str: A token string. If a token starts with the letter -letter- and is the beginning of a new word (i.e., follows a space),\n",
        "           that token is returned. Otherwise, a token is randomly selected based on `token_probs`.\n",
        "    \"\"\"\n",
        "    for token in tokens:\n",
        "        if \" \" in token[:2]: # we only want to select a words for new words (after a space)\n",
        "            for char in token:\n",
        "                if char.isalpha():\n",
        "                    if char.lower() == letter:\n",
        "                        return token\n",
        "                    else:\n",
        "                        break\n",
        "    return np.random.choice(tokens, 1, p=token_probs.numpy())[0]\n",
        "\n",
        "\n",
        "def feel_free_to_write_a_new_sampler(tokens: List, token_probs: torch.Tensor, *args, **kwargs) -> str:\n",
        "    raise NotImplementedError\n",
        "\n",
        "\n",
        "\n",
        "prompt = \"As the ship started to sway, the master-at-arms\"\n",
        "output = generate_custom(prompt, try_for_certain_letter, temperature=1, top_p=0.9, top_k=None, max_new_tokens=50)\n",
        "print(output)\n",
        "\n",
        "# try increasing temperature > 1, or near 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 880
        },
        "id": "FaCQdQoqKTvx",
        "outputId": "fcfe881a-c8ca-4bd8-d4bc-f0234233a2c5"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "As the ship started to sway, the master-at-arms began barking orders at his crew. He barked orders about building barricades, burying bodies, and burning buildings. But despite his best efforts, the battle was taking a toll on everyone. The boat was being battered by the rough waves, and the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizing the attention weights"
      ],
      "metadata": {
        "id": "yUi2xudvg7R3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Helper functions for getting and displaying attention weights\n",
        "\n",
        "def get_attn_weights(inputs, layer, head):\n",
        "    x = model.layers[0](**inputs)\n",
        "    for i in range(1, layer):\n",
        "        x = model.layers[i](x)\n",
        "    x = model.layers[layer].ln(x)\n",
        "    model.layers[layer].mixer\n",
        "    qkv = model.layers[layer].mixer.Wqkv(x)\n",
        "    qkv = rearrange(qkv, \"... (three h d) -> ... three h d\", three=3, d=model.layers[layer].mixer.head_dim)\n",
        "    qkv = model.layers[layer].mixer.rotary_emb(qkv)\n",
        "    batch_size, seqlen = qkv.shape[0], qkv.shape[1]\n",
        "    q, k, v = qkv.unbind(dim=2)\n",
        "    softmax_scale = 1.0 / math.sqrt(q.shape[-1])\n",
        "    scores = torch.einsum('bthd,bshd->bhts', q, k * softmax_scale)\n",
        "    causal_mask = torch.triu(torch.full(size=(seqlen, seqlen), fill_value=-10000.0, device=scores.device), 1)\n",
        "    scores = scores + causal_mask.to(dtype=scores.dtype)\n",
        "    attention = torch.softmax(scores, dim=-1, dtype=v.dtype)\n",
        "    output = torch.einsum('bhts,bshd->bthd', attention, v)\n",
        "    weights = attention[0, head].cpu()\n",
        "    return weights\n",
        "\n",
        "\n",
        "def display_attention_weights(inputs, layer, head, token_idx):\n",
        "    input_tokens = [tokenizer.decode(id) for id in inputs[\"input_ids\"][0]]\n",
        "    weights = get_attn_weights(inputs, layer, head)\n",
        "    with out:\n",
        "        fig, ax = plt.subplots(figsize=(3, 1*(len(input_tokens)//4)))\n",
        "        ax.axis('off')\n",
        "        tl = len(input_tokens)\n",
        "        ax.set_ylim(0, len(input_tokens))\n",
        "        ax.set_xlim(0, 10)\n",
        "        for i, token in enumerate(input_tokens):\n",
        "            ax.text(3, len(input_tokens)-i, token, ha='right', va='top')\n",
        "            ax.text(8, len(input_tokens)-i, token, ha='left', va='top')\n",
        "        ax.fill_between([0, 3.3], [tl-token_idx, tl-token_idx], [tl-token_idx-0.75, tl-token_idx-0.75], color='blue', alpha=0.4)\n",
        "        for i, weight in enumerate(weights[token_idx].cpu().tolist()):\n",
        "            ax.fill_between([7.7, 13], [tl-i, tl-i], [tl-i-0.75, tl-i-0.75], color='blue', alpha=math.sqrt(weight)*0.7)\n",
        "            ax.plot([3.35, 7.65], [tl-token_idx - 0.375, tl-i], c=\"blue\", alpha=math.sqrt(weight)*0.7, lw=0.5)\n",
        "        out.clear_output()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "def handler(_):\n",
        "    display_attention_weights(inputs, layer.value, head.value, token_idx.value)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38
        },
        "cellView": "form",
        "id": "U3N5P2plqaOX",
        "outputId": "4aaa12e8-ff68-40d2-b384-b8bd6271e429"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Select the Layer, Attention Head, and Token to view the attention weights\n",
        "layer = widgets.Dropdown(options=list(range(1, 24)), description=\"Layer\")\n",
        "head = widgets.Dropdown(options=list(range(0, 32)), description=\"Attn Head:\")\n",
        "token_idx = widgets.Dropdown(options=list(zip([tokenizer.decode(id) for id in inputs[\"input_ids\"][0]], list(range(len(inputs[\"input_ids\"][0]))))), description=\"Token:\")\n",
        "button = widgets.Button(description=\"Plot\")\n",
        "button.on_click(handler)\n",
        "\n",
        "out = widgets.Output()\n",
        "\n",
        "display(layer, head, token_idx, button)\n",
        "display(out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161,
          "referenced_widgets": [
            "2497edb100b04a6cb066e3e8acfb46de",
            "ea043bf41710443a8e9253c8eb1b0c4b",
            "377d738779c541a1b420db1c7ebeae45",
            "1b46cd9eae9d4ac3abc1f06802e5f10c",
            "114f30e96978466d8d9c4a32a59a4368",
            "be66ba2d35344b9686ef8c273acdd386",
            "78bfcf96f3fd4fc1a94b50ebb1f85ca7",
            "17dd6a02507c4ed2abd1d08ccf47836e",
            "477c5091664647cd85adeef11a2263e0",
            "0c6427bd9aaf4754aec9fe09145c10a2",
            "9db58afa7e8b4b36918b491c1010d141",
            "e02b4da4828847868e3bed9a18f301e4",
            "62f3216b0411442eb622382792dc1d39",
            "67e6b43832ba4da888b764d298d8117e"
          ]
        },
        "cellView": "form",
        "id": "X-w1l_PERmDv",
        "outputId": "7815935b-5e76-4e65-8128-5be8fdb3eb90"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Dropdown(description='Layer', options=(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, …"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2497edb100b04a6cb066e3e8acfb46de"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Dropdown(description='Attn Head:', options=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, …"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1b46cd9eae9d4ac3abc1f06802e5f10c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Dropdown(description='Token:', options=(('as', 0), (' it', 1), (' started', 2), (' to', 3), (' sway', 4), (','…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "78bfcf96f3fd4fc1a94b50ebb1f85ca7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Button(description='Plot', style=ButtonStyle())"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0c6427bd9aaf4754aec9fe09145c10a2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "62f3216b0411442eb622382792dc1d39"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Viewing the embedding dimension values across layers\n",
        "\n",
        "In the following visualization, you can see the embedding dimension being modified and adjusted over each layer.  Here we are only showing the first 5 heads worth of embedding dimension (64*5 = 320).  The red dashed lines represent separations in the values that each head attends to."
      ],
      "metadata": {
        "id": "75DscqqTROf9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Code to generate the following visualization\n",
        "# get the number of total heads and the head dimension\n",
        "_ = \"\"\"\n",
        "n_head = model.layers[1].mixer.n_head\n",
        "head_dim = model.layers[1].mixer.head_dim\n",
        "\n",
        "# get a color palette and shuffle it\n",
        "pal = sns.cubehelix_palette(5, rot=-.6, gamma=0.7, hue=0.7)\n",
        "random.shuffle(pal)\n",
        "\n",
        "# initialize the plot\n",
        "fig, ax = plt.subplots()\n",
        "_ = ax.set_ylim([-2, 2])\n",
        "_ = ax.set_xlabel(\"Embedding dimension position\")\n",
        "_ = ax.set_ylabel(\"Embedding value\")\n",
        "sns.despine()\n",
        "\n",
        "# calculate the x_values, y_values and colors for each bar initlaly\n",
        "x_vals = range(0, 5*head_dim)\n",
        "y_vals = [x[i*head_dim:i*head_dim+head_dim] for i in range(5)]\n",
        "heights = [i for head in y_vals for i in head]\n",
        "colors = [color for head in [[pal[i]]*head_dim for i in range(5)] for color in head]\n",
        "\n",
        "# add the bars, text, and dashed lines\n",
        "bars = plt.bar(x_vals, heights, color=colors)\n",
        "text_label = ax.text(0.7, 0.9, '', transform=ax.transAxes)\n",
        "for x_val in range(head_dim, 5*64, 64):\n",
        "    _ = ax.axvline(x=x_val, color='r', linestyle='--', lw=0.5)\n",
        "\n",
        "\n",
        "prev_heights = None\n",
        "interpolation_steps = 10\n",
        "\n",
        "def update(frame):\n",
        "    global prev_heights\n",
        "\n",
        "    real_frame = frame // interpolation_steps\n",
        "    interp = frame % interpolation_steps / interpolation_steps\n",
        "\n",
        "    x = model.layers[0](**inputs)\n",
        "    for i in range(1, real_frame):\n",
        "        x = model.layers[i](x)\n",
        "    x = model.layers[layer].ln(x)\n",
        "    x = model.layers[layer].mixer(x)\n",
        "    x = x[0, -1].tolist()\n",
        "    y_vals = [x[i * head_dim:i * head_dim + head_dim] for i in range(5)]\n",
        "    heights = [i for head in y_vals for i in head]\n",
        "\n",
        "    if prev_heights is not None:\n",
        "        # Perform the linear interpolation between the previous and current frame.\n",
        "        heights = [(1 - interp) * prev + interp * curr for prev, curr in zip(prev_heights, heights)]\n",
        "\n",
        "\n",
        "    for i, bar in enumerate(bars):\n",
        "        bar.set_height(heights[i])\n",
        "\n",
        "    # Update the text label\n",
        "    text_label.set_text(f'Model Layer: {real_frame + 1}')\n",
        "\n",
        "    # Store the current heights for the next frame\n",
        "    prev_heights = heights\n",
        "\n",
        "    return bars\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "# uncomment the following lines to run this cell\n",
        "# ani = animation.FuncAnimation(fig, update, frames=24 * interpolation_steps, blit=True, interval=400//interpolation_steps)\n",
        "# HTML(ani.to_html5_video())\n",
        "# ani.save('animation.mp4', writer='ffmpeg', fps=30)\n",
        "# from google.colab import files\n",
        "# files.download('animation.mp4')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38
        },
        "cellView": "form",
        "id": "9akgzRXM1Ubs",
        "outputId": "b63ae32a-874c-465e-e362-795ce2c28c69"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "HTML(f\"\"\"<video src=https://github.com/tim-a-davis/silly_little_language_modeling_thing_at_utd/raw/main/animation.mp4 width=700 controls loop/>\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 567
        },
        "id": "I1_SWYoh1Tuj",
        "outputId": "b64ea9c1-ea7c-4090-c575-c8f6683ced06"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<video src=https://github.com/tim-a-davis/silly_little_language_modeling_thing_at_utd/raw/main/animation.mp4 width=700 controls loop/>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tear Down"
      ],
      "metadata": {
        "id": "gmJKUNhEf5wI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "del model\n",
        "del outputs\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38
        },
        "id": "6R1yxBAk3krS",
        "outputId": "1fd5920a-c6e9-41d6-f7ed-7366048ea83e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chat-tuned Models"
      ],
      "metadata": {
        "id": "Pdo-Yni8ZpT-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\"teknium/Puffin-Phi-v2\", trust_remote_code=True, torch_dtype=torch.bfloat16)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"teknium/Puffin-Phi-v2\", trust_remote_code=True, torch_dtype=torch.bfloat16)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38
        },
        "id": "yhwi25jkjlC_",
        "outputId": "44b01e4e-4211-4ada-fc6f-7ea65ddeca9b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sysprompt = \"You are an assistant that gives helpful answers\\n\"\n",
        "inputs = tokenizer(f\"{sysprompt}USER: What is a master-at-arms?  Give me a short answer.\\nASSISTANT:\", return_tensors=\"pt\", return_attention_mask=False)\n",
        "outputs = model.generate(**inputs, max_new_tokens=120, temperature=0.7, do_sample=True, use_cache=True, repetition_penalty=1.2, eos_token_id=tokenizer.eos_token_id)\n",
        "text = tokenizer.batch_decode(outputs)[0]\n",
        "\n",
        "print(\"\\n\\nOutput:\\n\\n\", text)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Ny0_AxO6Zvsl",
        "outputId": "f773a8c4-e349-4d6c-9366-ec8c8afc3d58"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1417: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
            "  warnings.warn(\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Output:\n",
            "\n",
            " You are an assistant that gives helpful answers\n",
            "USER: What is a master-at-arms?  Give me a short answer.\n",
            "ASSISTANT: A master-at-arms is someone who has been specially trained to use weapons or armor for defensive purposes, often in combat situations. In simpler terms, they're like the \"goose\" of the battlefield – always vigilant and ready to protect their wielder from harm<|endoftext|>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5GMwyA3bh2cC"
      },
      "execution_count": 22,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "_pwlS5rkTAIh"
      ],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2497edb100b04a6cb066e3e8acfb46de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DropdownModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DropdownModel",
            "_options_labels": [
              "1",
              "2",
              "3",
              "4",
              "5",
              "6",
              "7",
              "8",
              "9",
              "10",
              "11",
              "12",
              "13",
              "14",
              "15",
              "16",
              "17",
              "18",
              "19",
              "20",
              "21",
              "22",
              "23"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "DropdownView",
            "description": "Layer",
            "description_tooltip": null,
            "disabled": false,
            "index": 0,
            "layout": "IPY_MODEL_ea043bf41710443a8e9253c8eb1b0c4b",
            "style": "IPY_MODEL_377d738779c541a1b420db1c7ebeae45"
          }
        },
        "ea043bf41710443a8e9253c8eb1b0c4b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "377d738779c541a1b420db1c7ebeae45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1b46cd9eae9d4ac3abc1f06802e5f10c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DropdownModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DropdownModel",
            "_options_labels": [
              "0",
              "1",
              "2",
              "3",
              "4",
              "5",
              "6",
              "7",
              "8",
              "9",
              "10",
              "11",
              "12",
              "13",
              "14",
              "15",
              "16",
              "17",
              "18",
              "19",
              "20",
              "21",
              "22",
              "23",
              "24",
              "25",
              "26",
              "27",
              "28",
              "29",
              "30",
              "31"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "DropdownView",
            "description": "Attn Head:",
            "description_tooltip": null,
            "disabled": false,
            "index": 0,
            "layout": "IPY_MODEL_114f30e96978466d8d9c4a32a59a4368",
            "style": "IPY_MODEL_be66ba2d35344b9686ef8c273acdd386"
          }
        },
        "114f30e96978466d8d9c4a32a59a4368": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be66ba2d35344b9686ef8c273acdd386": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "78bfcf96f3fd4fc1a94b50ebb1f85ca7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DropdownModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DropdownModel",
            "_options_labels": [
              "as",
              " it",
              " started",
              " to",
              " sway",
              ",",
              " the",
              " master",
              "-",
              "at",
              "-",
              "arms"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "DropdownView",
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "index": 0,
            "layout": "IPY_MODEL_17dd6a02507c4ed2abd1d08ccf47836e",
            "style": "IPY_MODEL_477c5091664647cd85adeef11a2263e0"
          }
        },
        "17dd6a02507c4ed2abd1d08ccf47836e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "477c5091664647cd85adeef11a2263e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0c6427bd9aaf4754aec9fe09145c10a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Plot",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_9db58afa7e8b4b36918b491c1010d141",
            "style": "IPY_MODEL_e02b4da4828847868e3bed9a18f301e4",
            "tooltip": ""
          }
        },
        "9db58afa7e8b4b36918b491c1010d141": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e02b4da4828847868e3bed9a18f301e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "62f3216b0411442eb622382792dc1d39": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_67e6b43832ba4da888b764d298d8117e",
            "msg_id": "",
            "outputs": []
          }
        },
        "67e6b43832ba4da888b764d298d8117e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}